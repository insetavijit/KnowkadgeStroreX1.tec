| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.4.6.1 The "Lost Context" Problem**      | Motivation                              | Why prompt \| llm loses the original input variables                          | Classic chains destroy input context; Passthrough saves it.     |
| **LC10.4.6.2 Branching for Retention**         | Strategy                                | Splitting flow so one branch keeps data, other processes it                   | Branch the flow: one path processes, one path preserves.        |
| **LC10.4.6.3 Post-Processing Access**          | Use Case                                | needing the question *after* the answer is generated (e.g., citations)        | Keep the question alive to verify the answer later.             |
| **LC10.4.6.4 Full State Objects**              | Architecture                            | Passing a single "State" dict through the entire pipeline                     | Pass a global State object through the whole chain.             |
| **LC10.4.6.5 Memory Management**               | Cleanup                                 | Dropping heavy context (docs) before returning final response                 | Prune the state at the end to return a clean response.          |

# Context Preservation: Don't Forget the Question

A common frustration in basic chains:
`Retrieval -> Prompt -> LLM -> Output (String)`

Great, you have the answer. But... what was the question? What were the documents? If you need to log them or verify sources, they are gone.

---

## 1. The "Lost Context" Problem

```python
# The "Lossy" Chain
chain = prompt | llm | parser

# Result: "Paris is the capital of France."
# We lost the prompt variables!
```

---

## 2. Branching for Retention (The "Tuple" Pattern)

You can output a tuple (or dict) containing the result AND the source.

```python
from operator import itemgetter

chain = (
    RunnablePassthrough.assign(context=retriever)
    | {
        "answer": prompt | llm | parser,
        "source_docs": itemgetter("context"), # Keep docs
        "original_question": itemgetter("question") # Keep question
    }
)
```

Now the result is a rich object:
```json
{
  "answer": "Paris...",
  "source_docs": [...],
  "original_question": "Capital of France?"
}
```

---

## 3. Post-Processing Access

Why care? **Citations**.

To cite sources, you need the Model's answer *and* the Documents.

```python
runnable = (
    RunnablePassthrough.assign(context=retriever)
    | RunnablePassthrough.assign(answer=generation_chain) 
    | citation_step # Has access to BOTH 'context' and 'answer'
)
```

The `citation_step` can now look at `x["answer"]` and matching it against `x["context"]`.

---

## 4. Full State Objects (LangGraph Lite)

For complex apps, define a State Schema.

```python
class State(TypedDict):
    q: str
    docs: list
    history: list
    ans: str

# Every step takes State and returns State (or partial state update)
```

Using `.assign()` repeatedly ensures you are effectively mutating this state object as it flows down the river.

---

## Quick Reference

| Pattern | Code |
|---------|------|
| **Preserve Source** | `{"out": chain, "in": Passthrough()}` |
| **Enrich Flow** | `.assign(out=chain)` |
| **Prune** | `chain | PickKeys` (Clean up at end) |
