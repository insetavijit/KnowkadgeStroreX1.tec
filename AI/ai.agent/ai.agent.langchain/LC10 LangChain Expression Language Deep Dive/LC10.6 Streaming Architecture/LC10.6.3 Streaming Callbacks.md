| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.6.3.1 CallbackHandler Interface**       | Legacy API                              | on_llm_new_token method for capturing tokens                                  | The old way to stream: on_llm_new_token callback.               |
| **LC10.6.3.2 Async Callbacks**                 | Integration                             | Passing AsyncCallbackHandler to ainvoke                                       | Use Async handlers for modern async apps.                       |
| **LC10.6.3.3 Streaming to StdOut**             | Debugging                               | StreamingStdOutCallbackHandler usage                                          | Use standard callback to print stream to console.               |
| **LC10.6.3.4 Passing Handlers**                | Invocation                              | chain.invoke(..., config={'callbacks': [handler]})                            | Pass handlers in the config dictionary.                         |
| **LC10.6.3.5 Limitations**                     | Why move to v2                          | Difficulty in async contexts, lack of event typing                            | Callbacks are harder to manage than astream_events iterators.   |

# Streaming Callbacks: The Legacy Pattern

Before `astream_events`, Callbacks were the *only* way to get tokens. You still see them often, and they are useful for logging side-effects (like streaming to a file).

---

## 1. on_llm_new_token

To stream, you implement a handler.

```python
from langchain_core.callbacks import BaseCallbackHandler

class MyStreamer(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs):
        print(token, end="", flush=True)

handler = MyStreamer()
chain.invoke(input, config={"callbacks": [handler]})
```

Note: This works even with `invoke()`! You don't need `stream()` to trigger callbacks. The LLM streams internally, and the callback catches it.

---

## 2. StreamingStdOutCallbackHandler

The built-in "Matrix Mode".

```python
from langchain.callbacks import StreamingStdOutCallbackHandler

chain = prompt | ChatOpenAI(streaming=True)

# Prints to console in real-time
chain.invoke("Tell me a story", config={"callbacks": [StreamingStdOutCallbackHandler()]})
```

---

## 3. Async Callbacks

If using `ainvoke`, you must use `AsyncCallbackHandler` to avoid blocking.

```python
class MyAsyncStreamer(AsyncCallbackHandler):
    async def on_llm_new_token(self, token: str, **kwargs):
        await websocket.send(token)
```

---

## 4. Why move to V2 (astream_events)?

Callbacks have "Inversion of Control". You pass a function, and the chain calls you. This makes it hard to:
1.  Manage state (variables) between tokens.
2.  Handle flow control (pause/resume).
3.  Process multiple streams easily.

`astream_events` gives control back to *you* via a standard `for` loop.

---

## Quick Reference

| Method | Usage |
|--------|-------|
| **Handler** | `BaseCallbackHandler` / `AsyncCallbackHandler` |
| **Event** | `on_llm_new_token` |
| **Trigger** | Works with `invoke` (if LLM is streaming=True) |
| **Status** | **Legacy**: Prefer `astream_events` for new code |
