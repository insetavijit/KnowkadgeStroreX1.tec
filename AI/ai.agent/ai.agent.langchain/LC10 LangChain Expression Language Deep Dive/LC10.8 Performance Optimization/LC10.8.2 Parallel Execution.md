| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.8.2.1 RunnableParallel Mechanics**      | Implicit vs Explicit                    | How dicts automatically become parallel; explicit Parallel usage              | Dicts in chains run their values in parallel.                   |
| **LC10.8.2.2 The "Gather" Pattern**            | Architecture                            | Splitting execution and gathering results                                     | Fork, Run, Join methods to aggregate results.                   |
| **LC10.8.2.3 CPU vs IO Parallelism**           | Bottlenecks                             | Threads for IO; Processes for CPU (using ProcessPoolExecutor)                 | Use Processes for CPU; Threads/Async for IO.                    |
| **LC10.8.2.4 Parallelizing Map**               | List Processing                         | Running a chain over a list of inputs in parallel                             | Use .map() to run a chain over every item in a list concurrently.|
| **LC10.8.2.5 Race Conditions**                 | Safety                                  | Ensuring shared state (if any) is thread-safe                                 | Avoid shared mutable state in parallel steps.                   |

# Parallel Execution: Wider Pipes

Batching is for "Many Inputs".
Parallel Execution is for "Many Steps" within a single input.

---

## 1. RunnableParallel Mechanics

The workhorse of concurrency.

```python
chain = RunnableParallel({
    "context": retriever,        # IO Bound (Network)
    "question": RunnablePassthrough()
})
```

While `retriever` fetches docs, `RunnablePassthrough` runs (instantly). If you had 5 retrievers, they would all run at once.

---

## 2. Parallelizing Map (.map())

If your input is a list `[doc1, doc2, doc3]` and you want to summarize each.

```python
summary_chain = (
    prompt | llm | parser
)

# Apply chain to list items in parallel
list_chain = summary_chain.map()

list_chain.invoke([doc1, doc2, doc3])
```

It outputs `[sum1, sum2, sum3]`.
This is conceptually similar to `batch`, but `batch` is called by the *Client*, `map` is a step *inside the Chain*.

---

## 3. CPU vs IO Parallelism

Standard Runnables use `ThreadPoolExecutor`. This is perfect for LLM calls (IO wait).

If you have a step that processes image embeddings (CPU heavy):

```python
# This blocks the GIL (Global Interpreter Lock)
def heavy_cpu(x):
    return compute_pi(1000000)

chain = RunnableLambda(heavy_cpu)
# .batch() will NOT be parallel because of GIL.
```

**Fix**: Wrap it in `ProcessPoolExecutor` inside a `RunnableLambda`.

---

## 4. The Fan-Out / Fan-In Topology

Common in "Research Agents".

1.  **Fan-Out**: Generate 3 sub-queries.
2.  **Parallel**: Search Google for each query.
3.  **Fan-In**: Merge 3 sets of results.

```python
chain = (
    generate_queries          # Returns [q1, q2, q3]
    | search_chain.map()      # Runs search 3x
    | merge_results           # Combines list
)
```

---

## Quick Reference

| Pattern | Method |
|---------|--------|
| **Branching** | `RunnableParallel({})` |
| **List Map** | `chain.map()` |
| **CPU Heavy** | Needs ProcessPool |
| **IO Heavy** | Default works fine |
