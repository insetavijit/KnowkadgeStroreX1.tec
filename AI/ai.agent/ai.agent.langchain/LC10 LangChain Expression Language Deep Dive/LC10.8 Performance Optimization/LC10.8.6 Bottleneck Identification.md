| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.8.6.1 The "Critical Path"**             | Theory                                  | The sequence of dependent steps that dictates min time                        | The critical path determines the minimum possible latency.      |
| **LC10.8.6.2 Serial vs Parallel**              | Analysis                                | Identifying steps that *could* be parallel but aren't                         | Anything not dependent on the previous step should be parallel. |
| **LC10.8.6.3 Large Payloads**                  | Data transfer                           | Passing 1MB strings between steps                                             | Moving huge strings in Python is surprisingly slow.             |
| **LC10.8.6.4 Embedding Latency**               | Hidden cost                             | Re-embedding the query 5 times in a loop                                      | Don't re-embed the same text multiple times.                    |
| **LC10.8.6.5 TCP Handshakes**                  | Network                                 | Re-creating Client objects for every invoke                                   | Reuse the Client object to keep connections alive.              |

# Bottleneck Identification: The Critical Path

Start optimization by drawing the graph.

`A -> B -> C` (Time = A+B+C)
`A -> (B || C) -> D` (Time = A + max(B,C) + D)

---

## 1. The Critical Path

In a complex RAG chain:

1.  Transform Query (0.5s)
2.  Retrieve Docs (1.0s)
3.  Rewrite Docs (2.0s)
4.  Generate Answer (5.0s)

Total: 8.5s.
The Critical Path is 1->2->3->4.
If we parallelize Step 2 (Retrieve) and Step 3 (Rewrite - if independent? No, rewrite usually *needs* docs).

Wait, maybe "Rewrite Docs" meant "Summarize Docs".
If we Summarize Docs *before* Generation, we save tokens but spend time.

**Optimization**:
Stream the Docs *directly* to generation without summarization? Faster Time-To-First-Token, but higher cost.

---

## 2. TCP Handshakes (Client Reuse)

**Bad**:
```python
def call_llm(x):
    output = ChatOpenAI().invoke(x) # Creates new HTTPS connection each time
    return output
```

**Good**:
```python
# Create once, reuse pool
llm = ChatOpenAI() 

def call_llm(x):
    return llm.invoke(x)
```

In high-throughput systems (thousands of calls/min), keeping the `httpx` client alive saves ~50ms per call.

---

## 3. Large Payloads

If you pass a 10MB PDF text through 10 RunnableLambdas, Python does memory allocation/copying.
**Fix**: Pass a *reference* (ID) or ensure you are mutating in place (rare in LCEL, usually copies).
Better yet, use a VectorStore so you don't pass the text until the final step.

---

## Quick Reference

| Bottleneck | Fix |
|------------|-----|
| **Serial Steps** | `RunnableParallel` |
| **Connection Setup** | Global Client |
| **Data Copying** | Pass References |
| **Embedding** | Cache it |
