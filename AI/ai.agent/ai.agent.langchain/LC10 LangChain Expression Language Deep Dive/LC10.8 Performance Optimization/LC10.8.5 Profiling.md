| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.8.5.1 LangSmith Timelines**             | Visual Profiling                        | Seeing the waterfall of execution spans                                       | LangSmith is the primary profiler for LCEL.                     |
| **LC10.8.5.2 Local Timers**                    | Ad-hoc                                  | Wrapping steps in time.time() lambdas                                         | Simple timers help isolate slow steps locally.                  |
| **LC10.8.5.3 Python cProfile**                 | Deep Dive                               | Analyzing CPU time (overhead) vs IO time                                      | Use cProfile to find non-LLM lag (e.g. text processing).        |
| **LC10.8.5.4 Token per Second (TPS)**          | Metric                                  | Measuring generation speed of the LLM                                         | TPS is the golden metric for LLM speed.                         |
| **LC10.8.5.5 Latency vs Throughput**           | Concepts                                | Time to first token vs Total tokens per minute                                | Optimizing for chat (latency) vs batch (throughput) differs.    |

# Profiling: Where is the time going?

"My chain is slow."
Is it the Retriver? The LLM? The JSON Parser? Or your internet connection?

---

## 1. LangSmith Timelines

The zero-effort profiler.
Set `LANGCHAIN_TRACING_V2=true`.

Look at the **Trace View**:
*   **Green Bar**: Active time.
*   **Gap**: Waiting.

If you see a 2s Retriever bar and a 5s LLM bar, your total latency is 7s.
If you see them overlapped in a Parallel block, your latency is 5s (max of both).

---

## 2. Local Timers (Decorator)

If you can't use LangSmith:

```python
import time

def timer(func):
    def wrapper(x):
        start = time.time()
        res = func(x)
        print(f"Step took {time.time() - start:.2f}s")
        return res
    return wrapper

# Wrap specific steps
timed_step = RunnableLambda(timer(my_func))
```

---

## 3. Token per Second (TPS)

For LLMs, raw time doesn't matter as much as **TPS**.
*   GPT-4: ~20 TPS
*   GPT-3.5: ~80 TPS

If you are getting 5 TPS on GPT-3.5, something is wrong (network throttling or huge prompt overhead).

---

## 4. cProfile (CPU overhead)

If the LLM responds in 0.1s but your chain takes 2s, you have **Python bloat**.

```bash
python -m cProfile -o output.pstats my_script.py
snakeviz output.pstats
```

Look for:
*   Excessive Pydantic validation.
*   Regex recompilation.
*   Document serialization loops.

---

## Quick Reference

| Tool | Focus |
|------|-------|
| **LangSmith** | Network/Architecture (Macro) |
| **cProfile** | CPU/Python (Micro) |
| **TPS** | Generator Health |
