| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.8.3.1 LLM Caching**                     | Global setting                          | Enabling global cache to avoid re-generating same text                        | LangChain has built-in global LLM caching.                      |
| **LC10.8.3.2 Embedding Caching**               | Cost saving                             | CacheBackedEmbeddings (Local file or Redis)                                   | Never re-compute embeddings; use CacheBackedEmbeddings.         |
| **LC10.8.3.3 InMemoryCache vs Redis**          | Scale                                   | RAM (Fast/Volatile) vs Redis (Shared/Persistent)                              | Use Redis for production caching.                               |
| **LC10.8.3.4 Conditional Caching**             | Strategy                                | Caching retrieval results but not generation (if temperature > 0)             | Cache deterministic steps like retrieval.                       |
| **LC10.8.3.5 Cache Invalidation**              | Maintenance                             | Clearing cache when data prompts change                                       | Caches must be cleared when prompts change.                     |

# Caching: The Fastest Code is No Code

The best way to optimize an LLM call (3 seconds) is to make it take 0 seconds.

---

## 1. LLM Caching (Global)

You can set this once at startup.

```python
from langchain.globals import set_llm_cache
from langchain.cache import InMemoryCache

set_llm_cache(InMemoryCache())

# First run: 3s
llm.invoke("Hello")

# Second run: 0.001s
llm.invoke("Hello")
```

**Note**: This applies to exactly matching prompts + settings.

---

## 2. Embedding Caching

Embeddings cost money and time. If you re-index the same documents, you pay twice.

```python
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore

store = LocalFileStore("./cache/")
cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embedder, store, namespace="vectors"
)
```

Now use `cached_embedder` exactly like `OpenAIEmbeddings()`.

---

## 3. Redis Caching (Production)

InMemoryCache dies when the script restarts. Redis persists.

```python
from langchain.cache import RedisCache
from redis import Redis

set_llm_cache(RedisCache(redis_=Redis(...)))
```

This also allows sharing cache between multiple uvicorn workers.

---

## 4. Specific Caching (Memoization)

If you have a slow Python function (not an LLM), use standard decorators.

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def slow_math(x):
    return x * x

runnable = RunnableLambda(slow_math)
```

**Warning**: Standard `lru_cache` is per-process. It does not share across workers.

---

## Quick Reference

| Type | Backend | Persistence |
|------|---------|-------------|
| **Memory** | Dict | No |
| **File** | Disk | Yes (Local) |
| **Redis** | Network | Yes (Shared) |
| **Similarity** | Vector DB | Fuzzy Match (Semantic Cache) |
