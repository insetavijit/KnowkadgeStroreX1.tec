| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.8.4.1 Generators as Data**              | Concept                                 | Passing a generator object instead of a list                                  | Pass generators, not lists, to save memory.                     |
| **LC10.8.4.2 Streaming Iterators**             | Memory                                  | Processing large datasets row-by-row                                          | Process one item at a time with iterators.                      |
| **LC10.8.4.3 Deferred Tool Execution**         | Agents                                  | The agent decides *if* to call a tool; it doesn't prepay the cost             | Tools are only run if the agent invokes them.                   |
| **LC10.8.4.4 RunnableGenerator**               | Implementation                          | Wiring generators into LCEL chains                                            | LCEL supports generator-in, generator-out.                      |
| **LC10.8.4.5 Lazy Loading Documents**          | ETL                                     | Loading 100k docs one by one instead of load()                                | Use .lazy_load() instead of .load() for massive docs.           |

# Lazy Evaluation: Just-in-Time Processing

Most default methods in Python (like `list()`) are eager: they load everything into memory.
Lazy methods (`yield`) compute only what is needed right now.

---

## 1. Lazy Loading Documents

**The Trap**:
```python
loader = CSVLoader("huge.csv")
docs = loader.load() # BOOM. 10GB RAM usage.
```

**The Fix**:
```python
docs_iter = loader.lazy_load()
# 0GB RAM usage. Reading file line by line.

for doc in docs_iter:
    chain.invoke(doc)
```

Always use `lazy_load` for datasets larger than RAM.

---

## 2. Generators through Chains

LCEL supports iterators naturally.

```python
def generate_numbers():
    for i in range(100):
        yield i

def process(i):
    return i * 2

# Running .map() on a generator is lazy!
chain = RunnableLambda(process).map()
# It returns a generator, it doesn't run yet.

result_iter = chain.invoke(generate_numbers())
print(next(result_iter)) # Now it runs for i=0
```

---

## 3. Deferred Tool Execution

In `RunnableParallel`, everything runs.

```python
# Eager Parallelism
chain = RunnableParallel({
    "a": expensive_step, # RUNS NOW
    "b": cheap_step      # RUNS NOW
})
```

If you only want `expensive_step` conditionally:

```python
# Lazy Branching (Router)
chain = RunnableBranch(
    (condition, expensive_step),
    cheap_step
)
```

The branch not taken is **never executed**.

---

## Quick Reference

| Method | Type | RAM Usage |
|--------|------|-----------|
| **load()** | List | High |
| **lazy_load()** | Iterator | Low |
| **map()** | Iterator | Low |
| **batch()** | List | High (Buffers all) |
