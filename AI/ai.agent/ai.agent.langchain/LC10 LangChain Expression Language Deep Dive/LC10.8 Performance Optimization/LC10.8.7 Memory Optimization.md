| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| **LC10.8.7.1 Generator Pipelines**             | Low Footprint                           | Using yield to keep only 1 item in RAM                                        | Generators prevent memory spikes by streaming data.             |
| **LC10.8.7.2 Clearing VRAM in Local LLMs**     | GPU                                     | Unloading models to avoid OOM                                                 | Manage GPU memory aggressively for local models.                |
| **LC10.8.7.3 Batch Size Tuning**               | Trade-off                               | Finding the batch size that fills RAM without crashing                        | Tune batch size to maximize throughput without OOM.             |
| **LC10.8.7.4 Pruning History**                 | Context Window                          | Reducing chat history to save tokens and RAM                                  | Trim older messages to save context limits.                     |
| **LC10.8.7.5 Pydantic & Copying**              | Overhead                                | V1 vs V2 Pydantic memory usage                                                | Pydantic v2 is much more memory efficient.                      |

# Memory Optimization: Don't Crash

In cloud containers (AWS Lambda, Fargate), you often have 512MB RAM. LangChain can eat that fast.

---

## 1. Generator Pipelines (Review)

Using `lazy_load` + `stream` ensures you never hold the whole dataset.

```python
# Streaming ETL
loader.lazy_load() | map_step | vectorstore.add_documents
```

If you use `loader.load()`, you hold 100% of data.

---

## 2. Pruning History (TrimMessages)

`RunnableWithMessageHistory` grows forever.
Use `trim_messages` strategy.

```python
from langchain_core.messages import trim_messages

trimmer = trim_messages(
    max_tokens=1000,
    strategy="last",
    token_counter=llm,
    include_system=True
)

chain = (
    RunnablePassthrough.assign(messages=itemgetter("history") | trimmer)
    | prompt | llm
)
```

This acts as a "Sliding Window" automatically.

---

## 3. Pydantic & Copying

LCEL relies heavily on Pydantic.
Every time you do `.dict()` or create a model, memory is allocated.
**Optimization**:
*   Upgrade to `langchain-core` (uses Pydantic v2, Rust-based, lighter).
*   Avoid converting to dicts unnecessarily inside loops.

---

## 4. Batch Size Tuning

If embedding 10,000 docs:
*   Batch 10,000: OOM (Out of Memory).
*   Batch 100: Safe.

Use standard chunking utilities:

```python
for i in range(0, len(docs), 100):
    process(docs[i:i+100])
```

---

## Quick Reference

| Resource | Optimization |
|----------|--------------|
| **RAM** | Generators / Lazy Loading |
| **GPU VRAM** | Quantization, Offloading |
| **Tokens** | `trim_messages` |
