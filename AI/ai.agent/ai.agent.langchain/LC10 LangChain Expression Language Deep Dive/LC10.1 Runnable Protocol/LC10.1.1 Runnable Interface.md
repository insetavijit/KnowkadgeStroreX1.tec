| **Subtopic**                                       | **Focus & Purpose**                      | **Key Concepts / Details**                                                        | **One-Line Recall**                                               |
| -------------------------------------------------- | ---------------------------------------- | --------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **LC10.1.1.1 Definition of Runnable**              | Define the Runnable abstraction          | Base interface, input-output contract, composable unit                            | A Runnable is the atomic unit of LCEL composition.                |
| **LC10.1.1.2 Core Problem Runnable Solves**        | Identify the primary gap                 | Unifying chains, prompts, models under one interface; consistent API              | Runnable provides a uniform interface for all LangChain components.|
| **LC10.1.1.3 Role in LCEL Architecture**           | Position Runnable in LCEL                | Foundation of pipe operator, enables composition, polymorphic execution           | Runnable is the backbone of LCEL's composability.                 |
| **LC10.1.1.4 Core Methods Overview**               | Introduce method signatures              | invoke(), batch(), stream(), ainvoke(), abatch(), astream()                       | Every Runnable implements invoke, batch, and stream methods.      |
| **LC10.1.1.5 Typical Implementations**             | Connect abstraction to practice          | PromptTemplate, ChatModel, OutputParser, RunnableLambda, custom Runnables         | All LangChain components are Runnable implementations.            |

# The Runnable Interface: LCEL's Foundational Contract

The Runnable interface isn't just another abstraction; it's the **unifying contract** that makes the entire LangChain Expression Language possible. By defining a consistent protocol for data transformation, it turns diverse components into interchangeable building blocks—each obeying the same rules, composable with any other.

---

## 1. The Fundamental Principle: One Interface to Rule Them All

At its core, **Runnable standardizes execution**.

Before LCEL, LangChain had chains, prompts, models, parsers—each with its own API. Connecting them required glue code. The Runnable interface eliminates this friction by guaranteeing every component speaks the same language.

Think of it like **electrical outlets**. Your laptop, phone, and lamp have different purposes, but they all use the same socket interface. You don't rewire your house for each device. Runnable is that socket.

**Key Insight**: The power isn't in what a Runnable *does*—it's in the guarantee that it *can be connected*.

---

## 2. The Core Problem: Interface Fragmentation

Building production LLM pipelines used to mean juggling incompatible APIs:

*   **Prompts** returned strings or message lists.
*   **Models** expected specific input formats.
*   **Parsers** had their own invocation patterns.
*   **Composition** required manual plumbing for every connection.

Runnable treats all components as **data transformers**. Input goes in, output comes out. The contract is simple: call `invoke()`, get a result. This prevents your codebase from becoming a tangled mess of adapter code.

---

## 3. Position in the LCEL Stack

Runnable occupies the **interface layer** of LCEL architecture.

```
┌─────────────────────────────────────────┐
│          Your LCEL Pipeline             │
│     (chain | prompt | model | parser)   │
├─────────────────────────────────────────┤
│           Runnable Interface            │
│   (invoke, batch, stream, async)        │
├─────────────────────────────────────────┤
│     Concrete Implementations            │
│  (ChatOpenAI, PromptTemplate, Lambda)   │
└─────────────────────────────────────────┘
```

This layering is your guarantee. When LangChain adds new model providers or introduces new components, they implement Runnable—and your existing pipelines just work.

---

## 4. The 6 Core Methods: The Complete Protocol

Every Runnable implementation must honor these methods. They form the **behavioral contract**.

### invoke(input) → Output
The synchronous, single-item execution. Send one input, receive one output.

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
result = llm.invoke("What is LCEL?")  # Returns AIMessage
```

### batch(inputs) → List[Output]
Process multiple inputs concurrently. Optimized for throughput.

```python
results = llm.batch(["Question 1", "Question 2", "Question 3"])
```

### stream(input) → Iterator[Output]
Yield outputs incrementally. Essential for real-time UIs.

```python
for chunk in llm.stream("Explain quantum computing"):
    print(chunk.content, end="", flush=True)
```

### ainvoke, abatch, astream
The async equivalents—same semantics, native async/await support.

```python
result = await llm.ainvoke("Async question")
```

---

## 5. The Type Contract: InputType and OutputType

Runnables are **generic** over their input and output types. This enables type checking and IDE autocompletion.

```python
from langchain_core.runnables import Runnable

class MyRunnable(Runnable[str, int]):
    def invoke(self, input: str) -> int:
        return len(input)
```

Understanding the type contract helps you:
*   Debug composition errors before runtime.
*   Build type-safe pipelines with Pydantic.
*   Leverage IDE support for complex chains.

---

## 6. Typical Implementations: The Runnable Family

Understanding which components are Runnables unlocks composition intuition:

| Component | InputType | OutputType |
|-----------|-----------|------------|
| `ChatPromptTemplate` | `dict` | `ChatPromptValue` |
| `ChatOpenAI` | `PromptValue` or `str` | `AIMessage` |
| `StrOutputParser` | `AIMessage` | `str` |
| `RunnableLambda` | Any | Any |
| `RunnablePassthrough` | Any | Same (passthrough) |

All of these can be piped together because they share the Runnable interface.

---

## 7. Verification: The Interface in Action

Confirm your understanding with a minimal composition test:

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Three Runnables, one pipeline
chain = (
    ChatPromptTemplate.from_template("Explain {topic} in one sentence")
    | ChatOpenAI()
    | StrOutputParser()
)

# The pipe operator works because all three are Runnables
response = chain.invoke({"topic": "the Runnable interface"})
print(response)
```

---

## Quick Reference

| Concept | Key Point |
|---------|-----------|
| **Runnable** | The universal interface for LCEL components |
| **invoke()** | Synchronous single-item execution |
| **batch()** | Concurrent multi-item execution |
| **stream()** | Incremental output iteration |
| **async methods** | ainvoke, abatch, astream for async contexts |
| **InputType/OutputType** | Generic type parameters for type safety |
| **Benefit** | **Composability**: Pipe any Runnable to any other |
