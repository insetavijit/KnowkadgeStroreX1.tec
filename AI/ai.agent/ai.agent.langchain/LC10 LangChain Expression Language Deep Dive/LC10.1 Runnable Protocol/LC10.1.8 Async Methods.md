| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.1.8.1 Async Method Overview**           | Introduce async variants                | ainvoke, abatch, astream; async/await; event loop                             | Every sync method has an async equivalent prefixed with 'a'.    |
| **LC10.1.8.2 When to Use Async**               | Choose sync vs async                    | Web servers, concurrent I/O, blocking vs non-blocking                         | Use async for web servers and concurrent I/O operations.        |
| **LC10.1.8.3 ainvoke Semantics**               | Async single execution                  | await ainvoke(), coroutine return, async context managers                     | ainvoke() is the async version of invoke().                     |
| **LC10.1.8.4 abatch Semantics**                | Async batch execution                   | Concurrent coroutines, asyncio.gather pattern, batch optimization             | abatch() runs batch items as concurrent coroutines.             |
| **LC10.1.8.5 astream and astream_events**      | Async streaming                         | Async iterators, async for, event streaming                                   | astream() yields chunks via async iterator.                     |

# Async Methods: Non-Blocking Execution

The async protocol (`ainvoke`, `abatch`, `astream`) mirrors the sync protocol but operates within Python's asyncio event loop. For web applications, API servers, and concurrent workloads, async is essential—it prevents your application from blocking while waiting for LLM responses.

---

## 1. The Async Protocol: Mirror Methods

Every synchronous method has an async counterpart:

```
┌─────────────────────────────────────────────────┐
│          Sync ←→ Async Mapping                  │
├─────────────────────────────────────────────────┤
│  invoke()   ←→   ainvoke()                      │
│  batch()    ←→   abatch()                       │
│  stream()   ←→   astream()                      │
└─────────────────────────────────────────────────┘
```

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

# Sync
result = llm.invoke("Hello")

# Async
result = await llm.ainvoke("Hello")
```

**Key Insight**: The async methods are not wrappers—they use native async HTTP clients for true non-blocking I/O.

---

## 2. When to Use Async: The Decision Framework

### Use Async When:

*   **Web Servers**: FastAPI, Starlette, aiohttp endpoints
*   **Concurrent Requests**: Multiple LLM calls that can run in parallel
*   **Real-Time Applications**: WebSocket handlers, event-driven systems
*   **High-Throughput APIs**: Serving many users simultaneously

### Use Sync When:

*   **Scripts**: One-off data processing, CLI tools
*   **Jupyter Notebooks**: Interactive exploration (though async works too)
*   **Simple Applications**: No concurrency requirements

```python
# ❌ Blocking in async context (BAD)
async def handle_request():
    result = llm.invoke("Query")  # Blocks the event loop!
    return result

# ✅ Non-blocking (GOOD)
async def handle_request():
    result = await llm.ainvoke("Query")  # Yields control to event loop
    return result
```

---

## 3. ainvoke: Async Single Execution

`ainvoke()` is the async equivalent of `invoke()`. It returns a coroutine that must be awaited.

```python
import asyncio
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

async def main():
    result = await llm.ainvoke("What is async programming?")
    print(result.content)

# Run the async function
asyncio.run(main())
```

### In Web Frameworks

```python
from fastapi import FastAPI

app = FastAPI()
llm = ChatOpenAI()

@app.get("/query")
async def query(q: str):
    result = await llm.ainvoke(q)
    return {"response": result.content}
```

---

## 4. abatch: Concurrent Batch Execution

`abatch()` processes multiple inputs as concurrent coroutines—true parallelism within the event loop.

```python
async def process_many():
    questions = [
        "What is Python?",
        "What is JavaScript?",
        "What is Rust?"
    ]
    
    # All three run concurrently (not sequentially!)
    results = await llm.abatch(questions)
    
    for q, r in zip(questions, results):
        print(f"Q: {q}\nA: {r.content}\n")

asyncio.run(process_many())
```

### Under the Hood

`abatch()` effectively does:

```python
# Conceptually (simplified)
async def abatch(self, inputs):
    coroutines = [self.ainvoke(x) for x in inputs]
    return await asyncio.gather(*coroutines)
```

### With Concurrency Limits

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(max_concurrency=5)
results = await llm.abatch(many_questions, config=config)
```

---

## 5. astream: Async Streaming

`astream()` returns an **async iterator**—use `async for` to consume chunks.

```python
async def stream_response():
    async for chunk in llm.astream("Explain quantum computing"):
        print(chunk.content, end="", flush=True)

asyncio.run(stream_response())
```

### In Web Applications (SSE)

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

async def generate(query: str):
    async for chunk in llm.astream(query):
        yield f"data: {chunk.content}\n\n"

@app.get("/stream")
async def stream_chat(q: str):
    return StreamingResponse(
        generate(q),
        media_type="text/event-stream"
    )
```

### astream_events: Fine-Grained Async Events

For complex chains, `astream_events()` provides detailed event streams:

```python
async def detailed_stream():
    async for event in chain.astream_events(
        {"topic": "AI"},
        version="v2"
    ):
        if event["event"] == "on_chat_model_stream":
            print(event["data"]["chunk"].content, end="")
        elif event["event"] == "on_chain_start":
            print(f"\n[Starting: {event['name']}]")

asyncio.run(detailed_stream())
```

---

## 6. Mixing Sync and Async: Interoperability

Sometimes you need to call async code from sync contexts (or vice versa).

### Async → Sync (when you must)

```python
import asyncio

def sync_function():
    # NOT recommended in async apps, but useful for scripts
    result = asyncio.run(llm.ainvoke("Query"))
    return result
```

### Sync → Async

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def call_sync_from_async():
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as pool:
        result = await loop.run_in_executor(
            pool, 
            lambda: llm.invoke("Query")  # Runs in thread pool
        )
    return result
```

**Best Practice**: Stay in one paradigm (sync or async) throughout your application.

---

## 7. Verification: Testing Async Runnables

```python
import pytest

@pytest.mark.asyncio
async def test_ainvoke():
    llm = ChatOpenAI()
    result = await llm.ainvoke("Hello")
    assert result.content is not None

@pytest.mark.asyncio
async def test_abatch():
    llm = ChatOpenAI()
    results = await llm.abatch(["Q1", "Q2"])
    assert len(results) == 2

@pytest.mark.asyncio
async def test_astream():
    llm = ChatOpenAI()
    chunks = [chunk async for chunk in llm.astream("Hello")]
    assert len(chunks) > 0
```

---

## Quick Reference

| Concept | Key Point |
|---------|-----------|
| **ainvoke()** | Async single execution, use with `await` |
| **abatch()** | Concurrent batch, all items run in parallel |
| **astream()** | Async iterator, use with `async for` |
| **astream_events** | Fine-grained event stream for complex chains |
| **When to Use** | Web servers, concurrent I/O, high-throughput |
| **Interop** | Avoid mixing; use `asyncio.run()` if needed |
| **Benefit** | **Scalability**: Handle many requests without blocking |
