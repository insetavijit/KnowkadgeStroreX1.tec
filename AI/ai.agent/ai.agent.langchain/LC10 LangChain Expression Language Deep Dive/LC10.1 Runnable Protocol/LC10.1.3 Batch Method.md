| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.1.3.1 Batch Semantics**                 | Define batch() behavior                 | List input, list output, concurrent execution                                 | batch() processes multiple inputs concurrently.                 |
| **LC10.1.3.2 Concurrency Control**             | Manage parallel execution               | max_concurrency config, thread pools, resource limits                         | Control parallelism with max_concurrency in config.             |
| **LC10.1.3.3 Order Preservation**              | Understand output ordering              | Results maintain input order, deterministic mapping                           | batch() output order matches input order.                       |
| **LC10.1.3.4 Error Strategies**                | Handle partial failures                 | return_exceptions flag, per-item error handling                               | Use return_exceptions=True for graceful partial failures.       |
| **LC10.1.3.5 Performance Optimization**        | Maximize throughput                     | Optimal batch sizes, API rate limits, latency trade-offs                      | Batch for throughput; tune size for your use case.              |

# The Batch Method: Concurrent Bulk Execution

The `batch()` method transforms sequential thinking into **parallel power**. Instead of invoking one item at a time, you process entire lists concurrently—dramatically reducing total execution time for bulk workloads.

---

## 1. The Fundamental Contract: List In, List Out

`batch()` is the plural form of `invoke()`: you pass a list of inputs, you get a list of outputs.

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
questions = [
    "What is Python?",
    "What is JavaScript?",
    "What is Rust?"
]

# All three processed concurrently
answers = llm.batch(questions)
# answers: [AIMessage(...), AIMessage(...), AIMessage(...)]
```

**Key Insight**: `batch()` is not just a loop over `invoke()`—it executes requests in parallel, respecting API rate limits.

---

## 2. Concurrency Control: The max_concurrency Lever

By default, `batch()` runs as many concurrent requests as possible. Use `max_concurrency` to throttle.

```
┌─────────────────────────────────────────────────┐
│           Concurrency Spectrum                  │
├─────────────────────────────────────────────────┤
│  max_concurrency=1   →  Sequential (no gain)    │
│  max_concurrency=5   →  5 parallel requests     │
│  max_concurrency=None →  Unlimited (default)    │
└─────────────────────────────────────────────────┘
```

```python
from langchain_core.runnables import RunnableConfig

# Limit to 3 concurrent API calls
config = RunnableConfig(max_concurrency=3)
results = llm.batch(questions, config=config)
```

### When to Limit Concurrency

*   **API Rate Limits**: Avoid 429 errors from providers
*   **Memory Constraints**: Large responses consume RAM
*   **Downstream Systems**: Don't overwhelm databases or services

---

## 3. Order Preservation: Deterministic Mapping

Despite parallel execution, **output order matches input order**. This is guaranteed.

```python
inputs = ["A", "B", "C"]
outputs = chain.batch(inputs)

# outputs[0] corresponds to inputs[0]
# outputs[1] corresponds to inputs[1]
# outputs[2] corresponds to inputs[2]
```

You never need to track which result came from which input—the indices align.

---

## 4. Error Strategies: Handling Partial Failures

What happens when one item in the batch fails? By default, the entire batch raises an exception.

### Option A: Fail Fast (Default)
```python
try:
    results = llm.batch(questions)
except Exception as e:
    # One failure kills the whole batch
    handle_error(e)
```

### Option B: Collect Exceptions
```python
results = llm.batch(
    questions,
    return_exceptions=True
)

for i, result in enumerate(results):
    if isinstance(result, Exception):
        print(f"Item {i} failed: {result}")
    else:
        print(f"Item {i} succeeded: {result.content}")
```

**When to Use return_exceptions=True**:
*   Processing user-uploaded documents (some may be malformed)
*   Querying multiple data sources (some may be unavailable)
*   Any scenario where partial success is acceptable

---

## 5. Performance Optimization: Tuning Batch Size

Larger batches aren't always faster. Consider these trade-offs:

| Batch Size | Pros | Cons |
|------------|------|------|
| Small (1-10) | Fast feedback, low memory | Underutilizes parallelism |
| Medium (10-50) | Good balance | Sweet spot for most APIs |
| Large (100+) | Maximum throughput | Memory spikes, timeout risk |

### Practical Heuristic

```python
# For OpenAI-like APIs, 10-20 concurrent requests is often optimal
OPTIMAL_BATCH_SIZE = 20

def process_large_dataset(items, chain):
    results = []
    for i in range(0, len(items), OPTIMAL_BATCH_SIZE):
        chunk = items[i:i + OPTIMAL_BATCH_SIZE]
        results.extend(chain.batch(chunk))
    return results
```

---

## 6. Batch vs Invoke Loop: The Performance Difference

Never use a for-loop when you can use `batch()`:

```python
# ❌ SLOW: Sequential execution
results = []
for q in questions:
    results.append(llm.invoke(q))  # Waits for each to complete

# ✅ FAST: Parallel execution
results = llm.batch(questions)  # All run concurrently
```

For 100 items with 1-second API latency:
*   Sequential loop: ~100 seconds
*   Batch (10 concurrent): ~10 seconds

---

## Quick Reference

| Concept | Key Point |
|---------|-----------|
| **batch()** | List input → List output, concurrent execution |
| **max_concurrency** | Config option to limit parallel requests |
| **Order Preservation** | Output indices match input indices |
| **return_exceptions** | Collect errors instead of failing entire batch |
| **Optimal Size** | 10-50 items per batch for most APIs |
| **vs Loop** | 10-100x faster than sequential invoke() |
| **Benefit** | **Throughput**: Process bulk data efficiently |
