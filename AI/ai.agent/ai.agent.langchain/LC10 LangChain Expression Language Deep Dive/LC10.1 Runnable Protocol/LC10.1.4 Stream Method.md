| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.1.4.1 Stream Semantics**                | Define stream() behavior                | Iterator output, incremental chunks, yielded tokens                           | stream() yields output incrementally as it's generated.         |
| **LC10.1.4.2 Chunk Types**                     | Understand streamed data                | AIMessageChunk, StringChunk, delta content                                    | Chunks are partial outputs that accumulate to final result.     |
| **LC10.1.4.3 Real-Time UI Integration**        | Build responsive interfaces             | Token-by-token display, SSE, WebSocket patterns                               | Stream enables real-time typing effect in UIs.                  |
| **LC10.1.4.4 Stream Events**                   | Advanced streaming control              | astream_events, on_llm_new_token, event types                                 | astream_events provides fine-grained streaming control.         |
| **LC10.1.4.5 Accumulating Results**            | Reconstruct final output                | Chunk concatenation, message accumulation, final assembly                     | Concatenate chunks to reconstruct the complete response.        |

# The Stream Method: Real-Time Output Delivery

The `stream()` method unlocks **progressive output**—instead of waiting for the complete response, you receive tokens as they're generated. This transforms static "loading..." experiences into dynamic, typing-effect interfaces that feel alive.

---

## 1. The Fundamental Contract: Yield As You Go

`stream()` returns an **iterator** that yields output chunks incrementally.

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

for chunk in llm.stream("Explain quantum entanglement"):
    print(chunk.content, end="", flush=True)
# Output appears word-by-word, not all at once
```

**Key Insight**: The total response is the same as `invoke()`—but you see it token-by-token instead of waiting for the complete answer.

---

## 2. Understanding Chunks: The Delta Pattern

Streamed output comes in **chunks**—partial pieces that combine to form the complete response.

```
┌──────────────────────────────────────────────────┐
│              Streaming Timeline                  │
├──────────────────────────────────────────────────┤
│  t=0ms    →  chunk: "Quantum"                    │
│  t=50ms   →  chunk: " entanglement"              │
│  t=100ms  →  chunk: " is"                        │
│  t=150ms  →  chunk: " a"                         │
│  t=200ms  →  chunk: " phenomenon..."             │
│  ...                                             │
│  t=2000ms →  (stream ends)                       │
└──────────────────────────────────────────────────┘
```

### Chunk Types by Runnable

| Runnable | Chunk Type | Content Access |
|----------|------------|----------------|
| `ChatOpenAI` | `AIMessageChunk` | `chunk.content` |
| `StrOutputParser` | `str` | Direct string |
| Chain with parser | Depends on final step | Varies |

```python
from langchain_core.messages import AIMessageChunk

for chunk in llm.stream("Hello"):
    assert isinstance(chunk, AIMessageChunk)
    print(f"Received: '{chunk.content}'")
```

---

## 3. Real-Time UI Integration

Streaming is essential for production chat interfaces. No one wants to stare at a spinner for 10 seconds.

### Pattern: Server-Sent Events (SSE)

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

async def generate_stream(query: str):
    async for chunk in llm.astream(query):
        yield f"data: {chunk.content}\n\n"

@app.get("/chat")
async def chat(query: str):
    return StreamingResponse(
        generate_stream(query),
        media_type="text/event-stream"
    )
```

### Pattern: WebSocket

```python
@app.websocket("/ws")
async def websocket_chat(websocket):
    await websocket.accept()
    query = await websocket.receive_text()
    
    async for chunk in llm.astream(query):
        await websocket.send_text(chunk.content)
```

---

## 4. Stream Events: Fine-Grained Control

For complex chains, `astream_events()` provides detailed event streams including intermediate steps.

```python
async for event in chain.astream_events(
    {"topic": "AI"},
    version="v2"
):
    kind = event["event"]
    
    if kind == "on_chat_model_stream":
        # Token from the LLM
        print(event["data"]["chunk"].content, end="")
    elif kind == "on_chain_start":
        # Chain execution began
        print(f"Starting: {event['name']}")
```

### Common Event Types

| Event | Meaning |
|-------|---------|
| `on_chain_start` | Chain execution began |
| `on_chain_end` | Chain execution completed |
| `on_chat_model_stream` | LLM yielded a token |
| `on_parser_start` | Parser began processing |
| `on_tool_start` | Tool invocation began |

---

## 5. Accumulating Results: Reconstructing the Full Response

Sometimes you need both streaming display AND the final complete message.

```python
from langchain_core.messages import AIMessageChunk

chunks = []
full_response = None

for chunk in llm.stream("Explain LCEL"):
    chunks.append(chunk)
    print(chunk.content, end="", flush=True)

# Reconstruct full message by adding chunks
full_response = chunks[0]
for chunk in chunks[1:]:
    full_response = full_response + chunk  # Chunks support addition

print(f"\n\nFull content: {full_response.content}")
```

### The Addition Pattern

`AIMessageChunk` objects can be added together to reconstruct the complete `AIMessage`:

```python
chunk1 = AIMessageChunk(content="Hello")
chunk2 = AIMessageChunk(content=" World")
full = chunk1 + chunk2  # AIMessageChunk(content="Hello World")
```

---

## 6. Stream in Chains: Propagation Behavior

In a chain, streaming behavior depends on the **final component**:

```python
chain = prompt | llm | parser

# If parser supports streaming → chunks from parser
# If parser doesn't stream → waits for llm, then outputs once
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="")
```

**Key Point**: `StrOutputParser` **does** support streaming. `PydanticOutputParser` **does not** (it needs the complete response to parse).

---

## Quick Reference

| Concept | Key Point |
|---------|-----------|
| **stream()** | Returns iterator, yields chunks incrementally |
| **Chunks** | Partial outputs (AIMessageChunk, str) |
| **Real-Time UI** | Essential for chat interfaces (SSE, WebSocket) |
| **astream_events** | Fine-grained events for complex chains |
| **Accumulation** | Add chunks together: `chunk1 + chunk2` |
| **Chain Behavior** | Final component determines streaming support |
| **Benefit** | **Responsiveness**: Users see output immediately |
