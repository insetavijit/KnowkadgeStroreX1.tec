| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC10.2.1.1 Pipeline Architecture**           | Define pipeline structure               | Linear flow, inputs → outputs, stages as transformers                         | Pipelines transform data through a sequence of Runnables.       |
| **LC10.2.1.2 Data Flow Management**            | Handle data between steps               | Context passing, schema matching, transformation logic                        | Match OutputType[N] to InputType[N+1] for smooth flow.          |
| **LC10.2.1.3 Modular Design**                  | Build robust pipelines                  | Small components, single responsibility, independent testing                  | Construct pipelines from small, testable modules.               |
| **LC10.2.1.4 Error Boundaries**                | Isolate failures                        | Fallbacks at stage level, global vs local error handling                      | Place error fallbacks at specific stages to prevent total fail. |
| **LC10.2.1.5 Visualization**                   | Inspect complex flows                   | LangSmith tracing, graph visualization, print debugging                       | Visualize pipelines to understand data flow and bottlenecks.    |

# Complex Pipelines: Building Reliable Workflows

Real-world applications are rarely a single "prompt → model" call. They are **pipelines** of data processing, augmentation, reasoning, and formatting. Understanding how to structure these complex flows is the difference between a demo and a product.

---

## 1. The Pipeline Architecture

A pipeline is a directed flow of data through a series of transformations.

```
┌─────────────────────────────────────────────────────────────┐
│                   Pipeline Stages                           │
├─────────────────────────────────────────────────────────────┤
│  Input  →  [Validation]  →  [Retrieval]  →  [Prompting]  →  │
│  [Reasoning]  →  [Formatting]  →  Output                    │
└─────────────────────────────────────────────────────────────┘
```

In LCEL, this is achieved via the connection of multiple Runnables.

```python
# A typical complex pipeline
pipeline = (
    validate_input      # RunnableLambda
    | retrieve_docs     # VectorStoreRetriever
    | format_context    # RunnableLambda
    | prompt            # ChatPromptTemplate
    | model             # ChatOpenAI
    | parse_output      # StrOutputParser
)
```

**Key Insight**: Each stage is independent. The pipeline only knows that stage N passes data to stage N+1.

---

## 2. Data Flow Management

The biggest challenge in complex pipelines is **schema mismatch**.

*   Step 1 returns a `List[Document]`
*   Step 2 expects a `str`

You need **glue code** (adapters) to bridge these gaps.

### The Adapter Pattern

Use `RunnableLambda` to reshape data between steps.

```python
def docs_to_string(docs):
    return "\n\n".join(d.page_content for d in docs)

# Connect retrieval (List[Doc]) to prompt (str)
chain = (
    retriever 
    | RunnableLambda(docs_to_string) 
    | prompt 
    | model
)
```

### The Passthrough Pattern

Sometimes step N needs data from step N-2. Use `RunnablePassthrough.assign()` to carry context forward.

```python
from langchain_core.runnables import RunnablePassthrough

# Input: {"question": "..."}
chain = (
    RunnablePassthrough.assign(context=retriever)  # Adds 'context' key
    | prompt  # Now has {"question": "...", "context": "..."}
    | model
)
```

---

## 3. Modular Design: The "Unix Pipes" Approach

Don't write one giant chain. Break it down.

### Monolithic (Bad)
```python
big_chain = (
    step1 | step2 | step3 | step4 | step5 ... | step20
)
# Hard to test, hard to debug
```

### Modular (Good)
```python
retrieval_chain = step1 | step2
reasoning_chain = step3 | step4
formatting_chain = step5 | step6

# Compose modules
final_chain = retrieval_chain | reasoning_chain | formatting_chain
```

**Benefit**: You can unit test `retrieval_chain` in isolation before connecting it to the rest.

---

## 4. Error Boundaries

In a long pipeline, if step 10 fails, the whole execution crashes. Use **local fallbacks** to create error boundaries.

```python
# If retrieval fails, fall back to a keyword search
robust_retriever = semantic_retriever.with_fallbacks([keyword_retriever])

pipeline = (
    robust_retriever
    | prompt
    | model
)
```

This ensures that a partial failure doesn't catastrophicallly recover—it degrades gracefully.

---

## 5. Visualization and Tracing

Complex pipelines are opaque. You cannot "see" exactly what data is flowing where just by reading the code.

### LangSmith Tracing

LangSmith renders your pipeline as a **graph**, showing the input/output of every single Runnable.

*   **Green**: Success
*   **Red**: Failure (click to see traceback)
*   **Latency**: How long each step took

### Graph Printing

You can ask LangChain to draw the ASCII graph of your chain:

```python
chain.get_graph().print_ascii()
```

```
   +-------------+
   |  Input      |
   +-------------+
          *
          |
   +-------------+
   |  Retriever  |
   +-------------+
          *
          |
   +-------------+
   |   Prompt    |
   +-------------+
```

---

## 6. Example: The RAG Pipeline

A classic complex pipeline:

```python
# 1. Standalone components
context = itemgetter("question") | retriever | format_docs
question = itemgetter("question")

# 2. Pipeline assembly
rag_chain = (
    {
        "context": context,  # Parallel branch 1
        "question": question # Parallel branch 2
    }
    | prompt
    | llm
    | StrOutputParser()
)
```

---

## Quick Reference

| Concept | Key Point |
|---------|-----------|
| **Pipeline** | Linear sequence of transformations |
| **Adapters** | Use Lambda to reshape data between steps |
| **Passthrough** | Persist context through the pipeline |
| **Modularity** | Compose small, testable chains into larger ones |
| **Fallbacks** | Wrap specific stages to contain errors |
| **Visualization** | Use `print_ascii()` or LangSmith to debug |
| **Benefit** | **Manageability**: Tame complexity by structuring flows |
