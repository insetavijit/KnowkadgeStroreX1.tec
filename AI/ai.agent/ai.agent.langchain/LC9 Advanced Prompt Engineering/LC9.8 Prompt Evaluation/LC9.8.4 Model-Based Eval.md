| **Subtopic**                                          | **Focus & Purpose**                          | **Key Concepts / Details**                                                      | **One-Line Recall**                                                 |
| ----------------------------------------------------- | -------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| **[[LC9.8.4.1 LLM-as-a-Judge]]**                      | Meta-evaluation                              | Asking GPT-4 to grade GPT-3.5's answer; flexible and nuanced                    | Use LLM as a Judge.                                                 |
| **[[LC9.8.4.2 Pairwise Comparison]]**                 | Preference ranking                           | "Which is better, A or B?"; easier for models than absolute scoring             | Compare pairs of outputs.                                           |
| **[[LC9.8.4.3 Reasoning Traces]]**                    | Explainable grading                          | Asking the judge to explain *why* it gave a score; auditable                    | Require reasoning for grades.                                       |
| **[[LC9.8.4.4 Bias Mitigation]]**                     | Objectivity                                  | Handling positional bias (preferring first option); shuffling order             | Mitigate judge bias.                                                |
| **[[LC9.8.4.5 Reference-Free Eval]]**                 | Real-world                                   | Grading without a gold standard; checking coherence/relevance only              | Evaluate without ground truth.                                      |
