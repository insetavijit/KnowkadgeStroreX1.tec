| **Subtopic**                                          | **Focus & Purpose**                          | **Key Concepts / Details**                                                      | **One-Line Recall**                                                 |
| ----------------------------------------------------- | -------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| **[[LC1.2.9.1 Tiktoken Library]]**                    | Use OpenAI's token counting library          | tiktoken install, encoding selection, cl100k_base, model-specific encodings     | Use tiktoken to count tokens for OpenAI models.                     |
| **[[LC1.2.9.2 Counting Tokens Before Requests]]**     | Pre-calculate token usage                    | Prompt token count, message serialization, input validation                     | Count tokens before sending to check context limits.                |
| **[[LC1.2.9.3 Context Window Limits]]**               | Understand model context capacity            | 4K, 8K, 128K windows, context length by model, overflow handling                | Stay within context window to avoid truncation.                     |
| **[[LC1.2.9.4 Cost Estimation]]**                     | Calculate API costs                          | Pricing per 1K tokens, input vs output pricing, budget tracking                 | Estimate costs by multiplying token count by price.                 |
| **[[LC1.2.9.5 Token Budgeting Strategies]]**          | Allocate tokens effectively                  | Prompt compression, chunking, summarization, priority allocation                | Budget tokens between prompt, context, and response.                |
