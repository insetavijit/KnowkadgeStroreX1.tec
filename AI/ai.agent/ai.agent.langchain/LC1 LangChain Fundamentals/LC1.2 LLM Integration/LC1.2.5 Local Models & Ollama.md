| **Subtopic**                                          | **Focus & Purpose**                          | **Key Concepts / Details**                                                      | **One-Line Recall**                                                 |
| ----------------------------------------------------- | -------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| **[[LC1.2.5.1 Running Local LLMs]]**                  | Understand local model deployment            | CPU/GPU requirements, memory considerations, latency trade-offs                 | Local LLMs offer privacy but require hardware resources.            |
| **[[LC1.2.5.2 Ollama Integration]]**                  | Use Ollama with LangChain                    | ChatOllama, Ollama model library, pulling models, configuration                 | Use ChatOllama() to connect to locally running Ollama models.       |
| **[[LC1.2.5.3 LlamaCpp Integration]]**                | Run GGUF models directly                     | LlamaCpp class, GGUF model files, n_ctx, n_gpu_layers                           | LlamaCpp runs quantized GGUF models directly in Python.             |
| **[[LC1.2.5.4 HuggingFace Local Models]]**            | Use HuggingFace transformers locally         | HuggingFacePipeline, model loading, tokenizers, device placement                | HuggingFacePipeline wraps transformers for local inference.         |
| **[[LC1.2.5.5 Local vs Cloud Trade-Offs]]**           | Compare deployment options                   | Privacy, cost, latency, capability, maintenance, scaling                        | Choose local for privacy/cost; cloud for capability/scale.          |
