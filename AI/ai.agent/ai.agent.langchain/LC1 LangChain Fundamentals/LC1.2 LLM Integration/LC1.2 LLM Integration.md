| Topic                                          | Focus & Purpose                                                                                                                                                     |
| ---------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **[[LC1.2.1 LLM Wrappers Overview]]**          | What LLM wrappers provide; abstraction over provider APIs; uniform interface for different models; wrapper design patterns. Abstraction layer.                      |
| **[[LC1.2.2 ChatModels vs LLMs]]**             | Difference between ChatModel and LLM classes; message-based vs completion-based interfaces; when to use each; migration considerations. Interface selection.        |
| **[[LC1.2.3 OpenAI Integration]]**             | Setting up ChatOpenAI; API key configuration; model selection (gpt-4, gpt-3.5-turbo); OpenAI-specific parameters; function calling. OpenAI provider.                |
| **[[LC1.2.4 Anthropic Integration]]**          | Setting up ChatAnthropic; Claude models; API configuration; Anthropic-specific features; message formatting differences. Anthropic provider.                        |
| **[[LC1.2.5 Local Models & Ollama]]**          | Running local LLMs; Ollama integration; LlamaCpp; HuggingFace local models; trade-offs between local and cloud. Self-hosted models.                                 |
| **[[LC1.2.6 API Configuration]]**              | API keys and environment variables; timeout settings; base URLs; retry configuration; proxy settings; secure credential handling. Connection setup.                 |
| **[[LC1.2.7 Model Parameters]]**               | Temperature; max_tokens; top_p; frequency_penalty; presence_penalty; stop sequences; parameter tuning strategies. Output control.                                   |
| **[[LC1.2.8 Streaming Responses]]**            | Enabling streaming; stream callbacks; token-by-token output; async streaming; real-time user experiences; streaming patterns. Live output.                          |
| **[[LC1.2.9 Token Counting]]**                 | tiktoken library; counting tokens before requests; context window limits; cost estimation; token budgeting; avoiding truncation. Resource management.               |
