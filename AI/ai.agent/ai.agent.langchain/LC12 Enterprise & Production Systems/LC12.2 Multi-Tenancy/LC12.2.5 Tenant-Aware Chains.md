| **Subtopic** | **Focus** | **Key Concepts** | **One-Line Recall** |
|---|---|---|---|
| **LC12.2.5.1 Context Injection** | Pattern | Passing tenant through chains | Inject tenant_id into chain context. |
| **LC12.2.5.2 Tenant Retrievers** | RAG | Custom tenant-filtered retrievers | Retrievers auto-filter by tenant. |
| **LC12.2.5.3 Tenant Memory** | State | Isolated conversation history | Each tenant has separate memory. |
| **LC12.2.5.4 Configurable Chains** | Customization | Per-tenant chain behavior | Customize chains per tenant. |
| **LC12.2.5.5 Multi-Tenant RAG** | Pattern | Complete tenant-aware RAG | Full RAG with tenant isolation. |

# Tenant-Aware Chains: LangChain Multi-Tenancy

Building LangChain components that respect tenant boundaries.

---

## 1. Tenant Context Injection

```python
from langchain_core.runnables import RunnablePassthrough, RunnableConfig

class TenantAwareChain:
    def __init__(self, llm, vector_store):
        self.llm = llm
        self.vector_store = vector_store
    
    def invoke(self, query: str, tenant_id: str):
        # Inject tenant context
        config = RunnableConfig(metadata={"tenant_id": tenant_id})
        
        chain = (
            {"query": RunnablePassthrough(), "tenant_id": lambda x: tenant_id}
            | self.retrieve_with_tenant
            | self.generate
        )
        
        return chain.invoke(query, config=config)
    
    def retrieve_with_tenant(self, inputs: dict):
        docs = self.vector_store.similarity_search(
            inputs["query"],
            filter={"tenant_id": inputs["tenant_id"]}
        )
        return {"docs": docs, "query": inputs["query"]}
    
    def generate(self, inputs: dict):
        return self.llm.invoke(f"Context: {inputs['docs']}\nQ: {inputs['query']}")
```

---

## 2. Tenant-Aware Retrievers

```python
from langchain_core.retrievers import BaseRetriever

class TenantRetriever(BaseRetriever):
    def __init__(self, vectorstore, tenant_id: str):
        self.vectorstore = vectorstore
        self.tenant_id = tenant_id
    
    def _get_relevant_documents(self, query: str) -> list:
        # Always filter by tenant
        return self.vectorstore.similarity_search(
            query,
            k=5,
            filter={"tenant_id": self.tenant_id}
        )

# Factory pattern
class RetrieverFactory:
    def create(self, tenant_id: str) -> TenantRetriever:
        return TenantRetriever(shared_vectorstore, tenant_id)

# Usage
@app.post("/query")
@require_tenant
async def query(tenant_id: str, request: QueryRequest):
    retriever = retriever_factory.create(tenant_id)
    docs = retriever.get_relevant_documents(request.query)
    return process(docs, request.query)
```

---

## 3. Tenant-Isolated Memory

```python
from langchain.memory import ChatMessageHistory

class TenantChatMemory:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def get_memory(self, tenant_id: str, conversation_id: str):
        key = f"memory:{tenant_id}:{conversation_id}"
        messages = self.redis.lrange(key, 0, -1)
        
        history = ChatMessageHistory()
        for msg in messages:
            history.add_message(json.loads(msg))
        
        return history
    
    def save_message(self, tenant_id: str, conversation_id: str, message):
        key = f"memory:{tenant_id}:{conversation_id}"
        self.redis.rpush(key, json.dumps(message.dict()))
        self.redis.expire(key, 86400 * 30)  # 30 days

# Usage in chain
def create_conversation_chain(tenant_id: str, conversation_id: str):
    memory = tenant_memory.get_memory(tenant_id, conversation_id)
    
    return ConversationChain(
        llm=llm,
        memory=ConversationBufferMemory(chat_memory=memory)
    )
```

---

## 4. Configurable Chains

```python
class TenantChainBuilder:
    def build(self, tenant_id: str):
        config = config_service.get_config(tenant_id)
        
        # Different LLM per tenant
        llm = self.get_llm(config.model)
        
        # Custom system prompt
        system_prompt = config.system_prompt or "You are a helpful assistant."
        
        # Custom retriever settings
        retriever = TenantRetriever(
            vectorstore,
            tenant_id,
            k=config.num_results or 5
        )
        
        # Build chain
        chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "{question}\n\nContext: {context}")
            ])
            | llm
        )
        
        return chain
```

---

## 5. Complete Multi-Tenant RAG

```python
class MultiTenantRAG:
    def __init__(self, vectorstore, llm_factory, config_service):
        self.vectorstore = vectorstore
        self.llm_factory = llm_factory
        self.config_service = config_service
    
    async def query(self, tenant_id: str, query: str, conversation_id: str = None):
        # Get tenant config
        config = self.config_service.get_config(tenant_id)
        
        # Tenant-specific LLM
        llm = self.llm_factory.get_llm(tenant_id)
        
        # Tenant-filtered retrieval
        docs = self.vectorstore.similarity_search(
            query,
            k=config.num_results,
            filter={"tenant_id": tenant_id}
        )
        
        # Tenant memory (if conversation)
        memory = None
        if conversation_id:
            memory = self.get_memory(tenant_id, conversation_id)
        
        # Generate with context
        context = "\n\n".join([doc.page_content for doc in docs])
        
        messages = [
            SystemMessage(content=config.system_prompt)
        ]
        
        if memory:
            messages.extend(memory.messages)
        
        messages.append(
            HumanMessage(content=f"Context:\n{context}\n\nQuestion: {query}")
        )
        
        response = await llm.ainvoke(messages)
        
        # Save to memory
        if memory:
            memory.add_user_message(query)
            memory.add_ai_message(response.content)
        
        return {
            "answer": response.content,
            "sources": [doc.metadata for doc in docs],
            "tenant_id": tenant_id
        }
```

---

## Quick Reference

| Component | Tenant-Aware Pattern |
|-----------|---------------------|
| **Retriever** | Filter by tenant_id |
| **Memory** | Namespace by tenant |
| **Chain** | Inject tenant context |
| **Config** | Load tenant settings |
