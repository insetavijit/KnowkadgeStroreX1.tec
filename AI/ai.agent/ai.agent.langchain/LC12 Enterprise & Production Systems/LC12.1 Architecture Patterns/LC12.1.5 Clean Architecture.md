| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
|---|---|---|---|
| **LC12.1.5.1 Layered Architecture** | Separation | Presentation, business, data layers | Separate concerns into distinct layers. |
| **LC12.1.5.2 Dependency Inversion** | Principle | Depend on abstractions, not concretions | High-level doesn't depend on low-level. |
| **LC12.1.5.3 Ports and Adapters** | Hexagonal | Input/output ports, adapters | Core logic independent of I/O. |
| **LC12.1.5.4 Test Isolation** | Testing | Testing without external dependencies | Mock external dependencies for tests. |
| **LC12.1.5.5 Plugin Architecture** | Extensibility | Pluggable components | Swap implementations via interfaces. |

# Clean Architecture: Testable LangChain Design

Building maintainable, testable LangChain applications.

---

## 1. Layered Architecture

```python
# Presentation Layer (API)
@app.post("/query")
async def query_endpoint(request: QueryRequest):
    result = await query_service.process(request.query)
    return {"answer": result}

# Application Layer (Use Cases)
class QueryService:
    def __init__(self, llm_gateway, document_repo):
        self.llm = llm_gateway
        self.documents = document_repo
    
    async def process(self, query: str) -> str:
        docs = await self.documents.retrieve(query)
        return await self.llm.generate(query, docs)

# Domain Layer (Business Logic)
class ConversationLogic:
    @staticmethod
    def should_escalate(messages: list) -> bool:
        # Pure business logic
        return any("urgent" in m.content.lower() for m in messages)

# Infrastructure Layer
class ChromaDocumentRepository:
    def __init__(self):
        self.vectorstore = Chroma(...)
```

---

## 2. Dependency Inversion

```python
from abc import ABC, abstractmethod

# Abstraction (interface)
class LLMGateway(ABC):
    @abstractmethod
    async def generate(self, prompt: str) -> str:
        pass

# High-level module depends on abstraction
class QueryUseCase:
    def __init__(self, llm: LLMGateway):
        self.llm = llm  # Depends on abstraction
    
    async def execute(self, query: str) -> str:
        return await self.llm.generate(query)

# Low-level implementation
class OpenAIGateway(LLMGateway):
    async def generate(self, prompt: str) -> str:
        llm = ChatOpenAI(model="gpt-4o")
        return llm.invoke(prompt).content

# Dependency injection
llm_gateway = OpenAIGateway()
use_case = QueryUseCase(llm=llm_gateway)
```

---

## 3. Ports and Adapters (Hexagonal)

```python
# Core (Domain)
class ConversationCore:
    def process_query(self, query: str, context: list) -> str:
        # Pure business logic
        return f"Response to: {query}"

# Input Port
class QueryPort(ABC):
    @abstractmethod
    def handle_query(self, query: str) -> str:
        pass

# Output Port
class DocumentPort(ABC):
    @abstractmethod
    def retrieve_documents(self, query: str) -> list:
        pass

# Input Adapter (REST API)
class RestAdapter:
    def __init__(self, query_port: QueryPort):
        self.query_port = query_port
    
    @app.post("/query")
    def endpoint(self, request):
        return self.query_port.handle_query(request.query)

# Output Adapter (Chroma)
class ChromaAdapter(DocumentPort):
    def retrieve_documents(self, query: str) -> list:
        return vectorstore.similarity_search(query)
```

---

## 4. Test Isolation

```python
# Mock LLM for testing
class MockLLM(LLMGateway):
    async def generate(self, prompt: str) -> str:
        return "mock response"

# Test without external dependencies
import pytest

@pytest.mark.asyncio
async def test_query_usecase():
    # Arrange
    mock_llm = MockLLM()
    use_case = QueryUseCase(llm=mock_llm)
    
    # Act
    result = await use_case.execute("test query")
    
    # Assert
    assert result == "mock response"

# No API calls in tests!
```

---

## 5. Plugin Architecture

```python
# Plugin interface
class EmbeddingProvider(ABC):
    @abstractmethod
    def embed(self, text: str) -> list[float]:
        pass

# Plugins
class OpenAIEmbeddings(EmbeddingProvider):
    def embed(self, text: str) -> list[float]:
        embeddings = OpenAIEmbeddings()
        return embeddings.embed_query(text)

class LocalEmbeddings(EmbeddingProvider):
    def embed(self, text: str) -> list[float]:
        model = SentenceTransformer('all-MiniLM-L6-v2')
        return model.encode(text).tolist()

# Configuration
def load_embedding_provider(config: dict) -> EmbeddingProvider:
    if config["provider"] == "openai":
        return OpenAIEmbeddings()
    else:
        return LocalEmbeddings()

# Swap at runtime
embedder = load_embedding_provider(app_config)
```

---

## Quick Reference

| Principle | Benefit |
|-----------|---------|
| **Layers** | Separation of concerns |
| **DI** | Testability, flexibility |
| **Ports/Adapters** | Framework independence |
| **Mocking** | Fast, reliable tests |
