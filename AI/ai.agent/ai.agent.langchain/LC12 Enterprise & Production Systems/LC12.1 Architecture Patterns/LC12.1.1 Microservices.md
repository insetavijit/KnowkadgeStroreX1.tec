| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ------------------------------------------------ | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC12.1.1.1 LangChain in Microservices**      | Integration                             | Building LangChain services                                                   | Each microservice owns specific LLM capability.                 |
| **LC12.1.1.2 Service Boundaries**              | Design                                  | Defining service responsibilities                                             | Align services with business capabilities.                      |
| **LC12.1.1.3 API Design**                      | Interfaces                              | Service API contracts                                                         | Define clear REST/gRPC interfaces.                              |
| **LC12.1.1.4 Service Communication**           | Integration                             | Inter-service messaging                                                       | Use async for non-critical, sync for critical.                  |
| **LC12.1.1.5 Service Discovery**               | Infrastructure                          | Locating services dynamically                                                 | Use Consul, Eureka, or K8s services.                            |

# Microservices: LangChain in Distributed Architecture

Building LangChain applications as microservices.

---

## 1. LangChain Microservices Pattern

```python
# Separate services for different concerns
"""
┌─────────────┐    ┌──────────────┐    ┌─────────────┐
│   Query     │───▶│   Document   │───▶│    LLM      │
│  Service    │    │   Service    │    │  Service    │
└─────────────┘    └──────────────┘    └─────────────┘
"""

# Query Service - handles user queries
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class QueryRequest(BaseModel):
    query: str
    user_id: str

@app.post("/query")
async def process_query(request: QueryRequest):
    # 1. Retrieve docs from Document Service
    docs = await document_service.retrieve(request.query)
    
    # 2. Call LLM Service with context
    response = await llm_service.generate(request.query, docs)
    
    return {"answer": response}

# Document Service - manages vector store
@app.post("/retrieve")
async def retrieve_documents(query: str):
    results = vectorstore.similarity_search(query, k=5)
    return {"documents": results}

# LLM Service - handles generation
@app.post("/generate")
async def generate_response(query: str, context: list):
    llm = ChatOpenAI(model="gpt-4o")
    response = llm.invoke(build_prompt(query, context))
    return {"response": response.content}
```

---

## 2. Service Boundaries

| Service | Responsibility | Owns |
|---------|----------------|------|
| **Query Router** | Route queries to specialists | Routing logic |
| **Document Service** | RAG retrieval | Vector store |
| **LLM Service** | Generation | Model API keys |
| **Memory Service** | Conversation history | Chat storage |
| **Agent Service** | Tool orchestration | Agent logic |

---

## 3. API Design

**RESTful Contracts**:
```python
# OpenAPI schema
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(
    title="LangChain Document Service",
    version="1.0.0"
)

class DocumentQuery(BaseModel):
    query: str
    k: int = 5
    filter: dict | None = None

class DocumentResult(BaseModel):
    documents: list[dict]
    metadata: dict

@app.post("/v1/retrieve", response_model=DocumentResult)
async def retrieve(query: DocumentQuery) -> DocumentResult:
    """Retrieve documents matching query."""
    pass
```

---

## 4. Service Communication

**Synchronous (when needed)**:
```python
import httpx

async def call_llm_service(prompt: str) -> str:
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://llm-service/v1/generate",
            json={"prompt": prompt},
            timeout=30.0
        )
        return response.json()["answer"]
```

**Asynchronous (preferred for non-critical)**:
```python
import pika

# Producer
connection = pika.BlockingConnection()
channel = connection.channel()
channel.queue_declare(queue='document_processing')

channel.basic_publish(
    exchange='',
    routing_key='document_processing',
    body=json.dumps({"doc_id": "123"})
)

# Consumer
def process_document(ch, method, properties, body):
    data = json.loads(body)
    # Process document
    vectorstore.add_documents([document])

channel.basic_consume(
    queue='document_processing',
    on_message_callback=process_document,
    auto_ack=True
)
```

---

## 5. Service Discovery (Kubernetes)

```yaml
# llm-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: llm-service
spec:
  selector:
    app: llm
  ports:
    - port: 80
      targetPort: 8000
```

```python
# Access via service name
LLM_SERVICE_URL = "http://llm-service/v1/generate"

# Kubernetes DNS resolves this automatically
```

---

## Quick Reference

| Pattern | When to Use |
|---------|-------------|
| **Sync HTTP** | Critical path, immediate response |
| **Async Queue** | Background processing, resilience |
| **gRPC** | High performance, internal APIs |
| **Service Mesh** | Complex inter-service comms |
