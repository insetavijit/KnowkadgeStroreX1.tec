| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC12.1.2.1 AWS Lambda**                      | Provider                                | Running LangChain on Lambda                                                   | Lambda works but watch cold starts.                             |
| **LC12.1.2.2 Cold Start Mitigation**           | Performance                             | Reducing cold start impact                                                    | Keep containers warm or use provisioned concurrency.            |
| **LC12.1.2.3 Serverless Frameworks**           | Tooling                                 | Serverless, SAM, CDK                                                          | Frameworks simplify deployment.                                 |
| **LC12.1.2.4 State Management**                | Challenge                               | Stateless limitations                                                         | Use external storage for state.                                 |
| **LC12.1.2.5 Cost Optimization**               | Economics                               | Serverless cost considerations                                                | Pay per invocation can be expensive for LLMs.                   |

# Serverless: LangChain in Serverless Functions

Running LangChain in serverless environments.

---

## 1. AWS Lambda Pattern

```python
# lambda_function.py
import json
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# Initialize outside handler (reused across warm starts)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

def lambda_handler(event, context):
    """Lambda handler for LangChain queries."""
    body = json.loads(event['body'])
    query = body.get('query')
    
    # Invoke LLM
    response = llm.invoke([HumanMessage(content=query)])
    
    return {
        'statusCode': 200,
        'body': json.dumps({'answer': response.content})
    }
```

**Challenges**:
| Challenge | Solution |
|-----------|----------|
| **Cold Starts** | Provisioned concurrency, keep warm |
| **Size Limits** | Layer dependencies, slim packages |
| **Timeout** | 15min max, chain async for longer |
| **Memory** | LLMs need RAM, use at least 2GB |

---

## 2. Cold Start Mitigation

```python
# Optimize imports
import json
import os

# Heavy imports inside handler (if not always needed)
def lambda_handler(event, context):
    # Import only when needed
    if event.get('action') == 'generate':
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(...)
    
    return process(event)
```

**Provisioned Concurrency**:
```yaml
# serverless.yml
functions:
  query:
    handler: handler.query
    provisionedConcurrency: 2  # Keep 2 warm
    timeout: 30
    memorySize: 2048
```

**Keep-Alive**:
```python
# CloudWatch Events - ping every 5 minutes
import boto3

lambda_client = boto3.client('lambda')

def keep_warm(event, context):
    lambda_client.invoke(
        FunctionName='langchain-query',
        InvocationType='Event',
        Payload=json.dumps({'warmup': True})
    )
```

---

## 3. Serverless Framework Example

```yaml
# serverless.yml
service: langchain-api

provider:
  name: aws
  runtime: python3.11
  region: us-east-1
  environment:
    OPENAI_API_KEY: ${env:OPENAI_API_KEY}
  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:GetItem
        - dynamodb:PutItem
      Resource: "arn:aws:dynamodb:*:*:table/chat-history"

functions:
  query:
    handler: handlers.query
    events:
      - http:
          path: query
          method: post
          cors: true
    timeout: 30
    memorySize: 2048

plugins:
  - serverless-python-requirements

custom:
  pythonRequirements:
    dockerizePip: true
    layer: true
```

---

## 4. State Management

Serverless is stateless - use external storage:

```python
import boto3
from langchain.memory import DynamoDBChatMessageHistory

dynamodb = boto3.resource('dynamodb')

def lambda_handler(event, context):
    session_id = event['session_id']
    
    # Persist memory externally
    memory = DynamoDBChatMessageHistory(
        table_name="chat-history",
        session_id=session_id
    )
    
    # Use in chain
    chain = ConversationChain(
        llm=llm,
        memory=ConversationBufferMemory(
            chat_memory=memory
        )
    )
    
    return chain.invoke(event['query'])
```

---

## 5. Cost Optimization

```python
# Cost considerations
"""
Lambda Pricing:
- $0.20 per 1M requests
- $0.0000166667 per GB-second

Example:
- 100k requests/month
- 2GB RAM, 5s duration
- = $0.20 + (100k * 5 * 2 * $0.0000166667) = ~$1.87/month

But LLM API calls are the real cost!
"""

# Optimize by caching
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_llm_call(query: str) -> str:
    """Cache responses for identical queries."""
    return llm.invoke(query).content

# Or use DynamoDB for persistent cache
def get_cached_response(query: str):
    table = dynamodb.Table('query-cache')
    result = table.get_item(Key={'query': query})
    return result.get('Item', {}).get('response')
```

---

## Quick Reference

| Platform | Best For |
|----------|----------|
| **Lambda** | AWS ecosystem, event-driven |
| **Cloud Functions** | GCP, Firebase integration |
| **Azure Functions** | Microsoft stack |
| **Vercel Functions** | Web apps, Next.js |
