| **Subtopic** | **Focus & Purpose** | **Key Concepts / Details** | **One-Line Recall** |
|---|---|---|---|
| **LC12.1.6.1 Horizontal Scaling** | Pattern | Adding more instances | Scale out by adding servers. |
| **LC12.1.6.2 Vertical Scaling** | Pattern | Bigger instances | Scale up with more resources. |
| **LC12.1.6.3 Autoscaling** | Automation | Dynamic scaling based on load | Automatically adjust capacity. |
| **LC12.1.6.4 Load Distribution** | Strategy | Distributing requests efficiently | Balance load across instances. |
| **LC12.1.6.5 Stateless Services** | Design | No server-side state | Stateless enables easy scaling. |

# Scalability: Growing LangChain Systems

Designing LangChain applications to scale.

---

## 1. Horizontal vs Vertical Scaling

| Approach | How | When | Limits |
|----------|-----|------|--------|
| **Horizontal** | Add instances | High traffic, distribute load | Eventually network/coordination |
| **Vertical** | Bigger machines | Memory-heavy tasks | Hardware limits |

```python
# Horizontal: Multiple API instances behind load balancer
"""
         ┌──────────────┐
         │ Load Balancer│
         └──────┬───────┘
        ┌───────┴────────┐
   ┌────▼────┐      ┌────▼────┐
   │ API-1   │      │ API-2   │
   │ LangChain│      │ LangChain│
   └─────────┘      └─────────┘
        │                │
        └────────┬───────┘
             ┌───▼────┐
             │ Shared │
             │ Storage│
             └────────┘
"""
```

---

## 2. Autoscaling Configuration

**Kubernetes HPA**:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langchain-api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langchain-api
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

**AWS Auto Scaling**:
```python
import boto3

autoscaling = boto3.client('autoscaling')

autoscaling.put_scaling_policy(
    AutoScalingGroupName='langchain-asg',
    PolicyName='scale-on-cpu',
    PolicyType='TargetTrackingScaling',
    TargetTrackingConfiguration={
        'PredefinedMetricSpecification': {
            'PredefinedMetricType': 'ASGAverageCPUUtilization'
        },
        'TargetValue': 70.0
    }
)
```

---

## 3. Load Distribution Strategies

```python
# Round Robin (simple)
class RoundRobinBalancer:
    def __init__(self, endpoints: list[str]):
        self.endpoints = endpoints
        self.current = 0
    
    def get_next(self) -> str:
        endpoint = self.endpoints[self.current]
        self.current = (self.current + 1) % len(self.endpoints)
        return endpoint

# Least Connections
class LeastConnectionsBalancer:
    def __init__(self):
        self.connections = {}  # endpoint -> count
    
    def get_next(self) -> str:
        return min(self.connections.items(), key=lambda x: x[1])[0]

# Client-side load balancing
import httpx

class LoadBalancedClient:
    def __init__(self, endpoints: list[str]):
        self.balancer = RoundRobinBalancer(endpoints)
    
    async def query(self, prompt: str) -> str:
        endpoint = self.balancer.get_next()
        async with httpx.AsyncClient() as client:
            response = await client.post(f"{endpoint}/query", json={"prompt": prompt})
            return response.json()["answer"]
```

---

## 4. Stateless Design

```python
# ❌ Bad: Server-side state
class StatefulAPI:
    def __init__(self):
        self.conversations = {}  # Stored in memory
    
    def add_message(self, session_id: str, message: str):
        self.conversations[session_id].append(message)
    # Problem: Different instance = different state

# ✅ Good: Stateless with external storage
class StatelessAPI:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def add_message(self, session_id: str, message: str):
        self.redis.lpush(f"conversation:{session_id}", message)
    # Works across instances
```

---

## 5. Scaling Patterns

```python
# Pattern 1: Read replicas for vector stores
"""
Write → Primary Chroma
Read  → Replica 1, Replica 2, Replica 3 (distributed reads)
"""

# Pattern 2: Caching layer
from redis import Redis

cache = Redis()

async def cached_query(query: str) -> str:
    # Check cache
    cached = cache.get(f"query:{hash(query)}")
    if cached:
        return cached.decode()
    
    # Generate and cache
    result = await llm.ainvoke(query)
    cache.setex(f"query:{hash(query)}", 3600, result.content)
    return result.content

# Pattern 3: Queue-based processing
"""
API → Queue → Worker Pool (scales independently)
"""
```

---

## Quick Reference

| Load | Scaling Strategy |
|------|------------------|
| **< 100 req/s** | Single instance |
| **100-1000 req/s** | 3-5 instances + load balancer |
| **> 1000 req/s** | Auto-scaling + caching |
| **Spiky** | Auto-scaling with buffer |
