| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC12.1.3.1 Event-Driven Chains**             | Pattern                                 | Chains triggered by events                                                    | Events trigger async chain execution.                           |
| **LC12.1.3.2 Message Queues**                  | Infrastructure                          | RabbitMQ, SQS, Kafka                                                          | Queues decouple producers and consumers.                        |
| **LC12.1.3.3 Event Sourcing**                  | Pattern                                 | Storing events as source of truth                                             | Event log is the source of truth.                               |
| **LC12.1.3.4 CQRS**                            | Pattern                                 | Command Query Responsibility Segregation                                      | Separate read and write models.                                 |
| **LC12.1.3.5 Async Processing**                | Implementation                          | Background processing patterns                                                | Process heavy tasks asynchronously.                             |

# Event-Driven Architecture: Async LangChain

Building event-driven LangChain systems.

---

## 1. Event-Driven Chain Pattern

```python
# Event Publisher
import boto3
import json

sns = boto3.client('sns')

def publish_query_event(query: str, user_id: str):
    """Publish query event for async processing."""
    event = {
        "event_type": "query_received",
        "data": {
            "query": query,
            "user_id": user_id,
            "timestamp": datetime.now().isoformat()
        }
    }
    
    sns.publish(
        TopicArn='arn:aws:sns:region:account:langchain-events',
        Message=json.dumps(event)
    )

# Event Consumer
def handle_query_event(event, context):
    """Process query event asynchronously."""
    data = json.loads(event['Records'][0]['Sns']['Message'])
    
    # Process with LangChain
    result = query_chain.invoke(data['data']['query'])
    
    # Publish result event
    publish_result_event(result, data['data']['user_id'])
```

---

## 2. Message Queue Integration

**With RabbitMQ**:
```python
import pika
import json

# Producer
connection = pika.BlockingConnection()
channel = connection.channel()

def queue_document_for_processing(doc_id: str):
    channel.basic_publish(
        exchange='langchain',
        routing_key='document.process',
        body=json.dumps({"doc_id": doc_id})
    )

# Consumer Worker
def process_documents():
    def callback(ch, method, properties, body):
        data = json.loads(body)
        
        # Process document with LangChain
        doc = load_document(data['doc_id'])
        chunks = text_splitter.split_documents([doc])
        vectorstore.add_documents(chunks)
        
        ch.basic_ack(delivery_tag=method.delivery_tag)
    
    channel.basic_consume(
        queue='document.process',
        on_message_callback=callback
    )
    
    channel.start_consuming()
```

---

## 3. Event Sourcing

Every state change is an event:

```python
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Event:
    event_type: str
    aggregate_id: str
    data: dict
    timestamp: datetime
    version: int

# Event Store
class ConversationEventStore:
    def __init__(self):
        self.events = []  # In practice: database
    
    def append(self, event: Event):
        self.events.append(event)
    
    def get_events(self, aggregate_id: str) -> list[Event]:
        return [e for e in self.events if e.aggregate_id == aggregate_id]

# Rebuild state from events
def rebuild_conversation(conversation_id: str) -> list:
    events = event_store.get_events(conversation_id)
    messages = []
    
    for event in events:
        if event.event_type == "message_sent":
            messages.append(event.data['message'])
        elif event.event_type == "message_edited":
            # Apply edit
            messages[event.data['index']] = event.data['new_content']
    
    return messages
```

---

## 4. CQRS Pattern

Separate read and write paths:

```python
# Write Model (Commands)
class QueryCommandHandler:
    def handle_query(self, query: str, user_id: str):
        # Process and store events
        event = Event(
            event_type="query_received",
            aggregate_id=user_id,
            data={"query": query},
            timestamp=datetime.now(),
            version=1
        )
        event_store.append(event)
        
        # Process async
        process_query_async(event)

# Read Model (Queries)
class ConversationQueryHandler:
    def __init__(self):
        # Denormalized read-optimized store
        self.read_db = {}
    
    def get_conversation(self, conversation_id: str):
        """Fast read from optimized store."""
        return self.read_db.get(conversation_id, [])
    
    def update_from_events(self, events: list[Event]):
        """Update read model from event stream."""
        for event in events:
            if event.event_type == "response_generated":
                conversation_id = event.aggregate_id
                if conversation_id not in self.read_db:
                    self.read_db[conversation_id] = []
                self.read_db[conversation_id].append(event.data)
```

---

## 5. Async Processing with Celery

```python
from celery import Celery

app = Celery('langchain_tasks', broker='redis://localhost:6379')

@app.task
def process_document_async(doc_id: str):
    """Process document in background."""
    doc = load_document(doc_id)
    chunks = text_splitter.split_documents([doc])
    vectorstore.add_documents(chunks)
    
    return {"status": "completed", "chunks": len(chunks)}

@app.task
def generate_summary_async(text: str):
    """Generate summary asynchronously."""
    chain = summary_chain
    return chain.invoke({"text": text})

# Trigger async task
from main import process_document_async
result = process_document_async.delay("doc_123")

# Check status
if result.ready():
    print(result.get())
```

---

## Quick Reference

| Pattern | Use Case |
|---------|----------|
| **Events** | Loose coupling, audit trail |
| **Queues** | Reliable async processing |
| **Event Sourcing** | Complete history, replay |
| **CQRS** | Scale reads separately |
