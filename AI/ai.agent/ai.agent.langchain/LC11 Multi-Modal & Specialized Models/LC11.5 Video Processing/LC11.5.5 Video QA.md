| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.5.5.1 Video QA Basics**                 | Simple QA                               | Answering questions about video content                                       | Ask questions, get answers about video.                         |
| **LC11.5.5.2 Multi-Frame Context**             | Comprehensive                           | Providing enough frames for context                                           | More frames = better temporal understanding.                    |
| **LC11.5.5.3 Timestamp-Aware QA**              | Precision                               | Questions about specific times                                                | Include timestamps for time-specific QA.                        |
| **LC11.5.5.4 Video RAG**                       | Large-scale                             | RAG pattern for large video collections                                       | Index video content for semantic search.                        |
| **LC11.5.5.5 Conversational Video QA**         | Multi-turn                              | Follow-up questions about same video                                          | Maintain context for multi-turn video QA.                       |

# Video QA: Answering Questions About Videos

Combining visual understanding with question answering.

---

## 1. Video QA Basics

Simple question answering about video content:

```python
def video_qa(video_path: str, question: str, num_frames: int = 10) -> str:
    """Answer a question about a video."""
    
    # Extract representative frames
    frames = extract_frames(video_path, sample_rate=get_sample_rate(video_path, num_frames))
    
    content = [
        {"type": "text", "text": f"These are {len(frames)} frames from a video."},
    ]
    
    for i, frame in enumerate(frames):
        content.append({"type": "image_url", "image_url": {"url": encode(frame)}})
    
    content.append({"type": "text", "text": f"Question: {question}"})
    
    return llm.invoke([HumanMessage(content=content)]).content

# Usage
answer = video_qa("tutorial.mp4", "What software is being demonstrated?")
```

---

## 2. Multi-Frame Context

Balancing frame count with context:

| Frames | Context | Token Cost | Use Case |
|--------|---------|------------|----------|
| **3-5** | Low | Low | Simple questions |
| **10-15** | Medium | Medium | Action/process questions |
| **20-30** | High | High | Complex temporal questions |

```python
def adaptive_frame_selection(video_path: str, question: str) -> list:
    """Choose frame count based on question type."""
    
    # Simple questions need fewer frames
    if any(kw in question.lower() for kw in ["what is", "who is", "describe"]):
        return extract_frames(video_path, num_frames=5)
    
    # Temporal questions need more frames
    if any(kw in question.lower() for kw in ["when", "how long", "sequence"]):
        return extract_frames(video_path, num_frames=20)
    
    # Default
    return extract_frames(video_path, num_frames=10)
```

---

## 3. Timestamp-Aware QA

For precise time-based questions:

```python
def timestamped_video_qa(video_path: str, question: str):
    """QA with timestamp context."""
    
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    cap.release()
    
    # Extract with timestamps
    frames_data = []
    for i, frame_path in enumerate(extract_frames(video_path)):
        frame_num = i * sample_rate
        timestamp = frame_num / fps
        frames_data.append((timestamp, frame_path))
    
    content = [
        {"type": "text", "text": "Video frames with timestamps:"}
    ]
    
    for ts, frame in frames_data:
        content.append({"type": "text", "text": f"[{ts:.1f}s]"})
        content.append({"type": "image_url", "image_url": {"url": encode(frame)}})
    
    content.append({
        "type": "text", 
        "text": f"Question: {question}\nInclude specific timestamps in your answer."
    })
    
    return llm.invoke([HumanMessage(content=content)]).content
```

---

## 4. Video RAG

For large video collections:

```python
class VideoRAG:
    def __init__(self):
        self.embeddings = OpenCLIPEmbeddings()
        self.vectorstore = Chroma(embedding_function=self.embeddings)
    
    def index_video(self, video_path: str):
        """Index video frames for retrieval."""
        frames = extract_frames(video_path, num_frames=30)
        
        docs = [
            Document(
                page_content="",
                metadata={
                    "video": video_path,
                    "frame_num": i,
                    "timestamp": i * (get_duration(video_path) / 30)
                }
            )
            for i, frame in enumerate(frames)
        ]
        
        self.vectorstore.add_documents(docs)
    
    def query(self, question: str, k: int = 5):
        """Find relevant video moments."""
        results = self.vectorstore.similarity_search(question, k=k)
        
        # Group by video
        video_frames = {}
        for doc in results:
            video = doc.metadata["video"]
            if video not in video_frames:
                video_frames[video] = []
            video_frames[video].append(doc)
        
        # Answer using retrieved frames
        return self.answer_with_frames(question, video_frames)
```

---

## 5. Conversational Video QA

Multi-turn dialogue about a video:

```python
class VideoConversation:
    def __init__(self, video_path: str):
        self.frames = extract_frames(video_path, num_frames=15)
        self.history = []
    
    def ask(self, question: str) -> str:
        # Include video frames only on first turn
        if not self.history:
            content = [{"type": "text", "text": "Video frames:"}]
            for frame in self.frames:
                content.append({"type": "image_url", "image_url": {"url": encode(frame)}})
            content.append({"type": "text", "text": f"Question: {question}"})
            self.history.append(HumanMessage(content=content))
        else:
            self.history.append(HumanMessage(content=question))
        
        response = llm.invoke(self.history)
        self.history.append(response)
        
        return response.content

# Usage
conv = VideoConversation("demo.mp4")
print(conv.ask("What is this video about?"))
print(conv.ask("What happens at the end?"))
print(conv.ask("How many people appear?"))
```

---

## Quick Reference

| Pattern | Use Case |
|---------|----------|
| **Basic QA** | Single question about video |
| **Timestamped** | Time-specific questions |
| **Video RAG** | Searching across video library |
| **Conversational** | Multi-turn exploration |
