| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.5.2.1 Frame-Based Understanding**       | Approach                                | Analyzing videos through representative frames                                | Analyze video by processing key frames.                         |
| **LC11.5.2.2 Native Video Models**             | Direct input                            | Gemini's video input capability                                               | Gemini accepts video files directly.                            |
| **LC11.5.2.3 Action Recognition**              | Motion                                  | Detecting what's happening in video                                           | VLMs can describe actions across frames.                        |
| **LC11.5.2.4 Scene Description**               | Context                                 | Describing overall video content                                              | Describe scenes, settings, and contexts.                        |
| **LC11.5.2.5 Object Tracking**                 | Following subjects                      | Tracking objects across frames                                                | Track objects through video sequences.                          |

# Video Understanding: Making Sense of Moving Images

Going beyond individual frames to understand video content.

---

## 1. Frame-Based Understanding

Send multiple frames to VLM:

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

def understand_video_frames(frame_paths: list[str], question: str) -> str:
    llm = ChatOpenAI(model="gpt-4o")
    
    content = [
        {"type": "text", "text": f"These are {len(frame_paths)} frames from a video, shown in order."}
    ]
    
    for i, path in enumerate(frame_paths):
        content.append({"type": "text", "text": f"Frame {i+1}:"})
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{encode_image(path)}"}
        })
    
    content.append({"type": "text", "text": f"\nQuestion: {question}"})
    
    response = llm.invoke([HumanMessage(content=content)])
    return response.content
```

---

## 2. Native Video Input (Gemini)

Gemini 1.5 accepts video directly:

```python
from langchain_google_genai import ChatGoogleGenerativeAI
import google.generativeai as genai

# Upload video to Gemini
video_file = genai.upload_file("video.mp4")

# Wait for processing
while video_file.state.name == "PROCESSING":
    time.sleep(2)
    video_file = genai.get_file(video_file.name)

# Query with video
llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro")

response = llm.invoke([
    HumanMessage(content=[
        {"type": "text", "text": "Describe what happens in this video."},
        {"type": "file", "file_uri": video_file.uri}
    ])
])
```

---

## 3. Action Recognition

Detect activities across frames:

```python
action_prompt = """
Analyze these video frames in sequence:

1. What action or activity is taking place?
2. How does it progress from start to end?
3. Who/what are the main subjects?
4. Describe any significant movements or changes.

Frames are shown in chronological order.
"""

actions = understand_video_frames(frames, action_prompt)
```

**Common detectable actions**:
- Walking, running, jumping
- Cooking, eating
- Speaking, gesturing
- Sports activities
- Work activities

---

## 4. Scene Description

Overall video context:

```python
scene_prompt = """
Based on these video frames:

1. Setting: Where does this take place?
2. Time: Is it day/night? What season might it be?
3. Subjects: Who or what appears in the video?
4. Mood: What is the overall atmosphere?
5. Context: What type of video is this (tutorial, vlog, security, etc)?
"""
```

---

## 5. Object Tracking (Simplified)

Track objects across frames:

```python
tracking_prompt = """
In these sequential frames, track the [red car]:

For each frame, note:
- Is the object visible?
- Where in the frame is it? (e.g., left, center, right)
- Any changes in size? (getting closer/farther)
- Any changes in orientation?
"""

# More precise tracking requires specialized models (SORT, DeepSORT)
```

**For precise tracking**, use dedicated tools:
```python
# pip install supervision

import supervision as sv

tracker = sv.ByteTrack()

for frame in frames:
    detections = detector(frame)  # YOLO
    tracked = tracker.update_with_detections(detections)
```

---

## Quick Reference

| Task | Approach |
|------|----------|
| **General Understanding** | Multi-frame VLM |
| **Long Video** | Gemini native |
| **Action Detection** | Sequential frame analysis |
| **Precise Tracking** | SORT/DeepSORT + YOLO |
