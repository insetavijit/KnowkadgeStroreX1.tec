| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.5.6.1 Audio Extraction**                | Source                                  | Extracting audio track from video                                             | Extract audio with FFmpeg for transcription.                    |
| **LC11.5.6.2 Transcript + Frames**             | Combined                                | Using both audio transcript and visual frames                                 | Combine transcript and frames for full context.                 |
| **LC11.5.6.3 Speaker Identification**          | Who's talking                           | Matching speech to faces/people                                               | Link speakers to visual subjects.                               |
| **LC11.5.6.4 Subtitle Generation**             | Accessibility                           | Creating accurate subtitles with timing                                       | Generate subtitles from audio + video.                          |
| **LC11.5.6.5 Audio-Visual Alignment**          | Synchronization                         | Matching transcript to visual content                                         | Align transcript text to visual moments.                        |

# Transcript Integration: Audio + Video Understanding

Videos have sound. Use it.

---

## 1. Audio Extraction

Pull audio from video:

```python
import subprocess

def extract_audio(video_path: str, output_path: str = None) -> str:
    """Extract audio track from video."""
    if output_path is None:
        output_path = video_path.rsplit(".", 1)[0] + ".mp3"
    
    cmd = [
        "ffmpeg", "-i", video_path,
        "-vn",  # No video
        "-acodec", "libmp3lame",
        "-q:a", "2",  # Quality
        output_path
    ]
    subprocess.run(cmd, check=True, capture_output=True)
    
    return output_path

# Usage
audio_path = extract_audio("lecture.mp4")
transcript = transcribe(audio_path)
```

---

## 2. Combined Analysis

Use both modalities:

```python
def analyze_video_full(video_path: str, question: str) -> str:
    """Analyze video using both visual and audio content."""
    
    # Extract and transcribe audio
    audio_path = extract_audio(video_path)
    transcript = transcribe(audio_path)
    
    # Extract frames
    frames = extract_frames(video_path, num_frames=10)
    
    # Combined prompt
    content = [
        {"type": "text", "text": "Analyze this video using both visuals and transcript."},
        {"type": "text", "text": f"TRANSCRIPT:\n{transcript}"},
        {"type": "text", "text": "KEY FRAMES:"},
    ]
    
    for frame in frames:
        content.append({"type": "image_url", "image_url": {"url": encode(frame)}})
    
    content.append({"type": "text", "text": f"Question: {question}"})
    
    return llm.invoke([HumanMessage(content=content)]).content
```

---

## 3. Speaker Identification

Match voices to faces:

```python
def identify_speakers(video_path: str):
    """Identify who is speaking when."""
    
    # 1. Diarize audio (who speaks when)
    diarization = diarize_audio(extract_audio(video_path))
    
    # 2. Extract face frames during speech segments
    speaker_faces = {}
    for segment in diarization:
        speaker_id = segment["speaker"]
        mid_time = (segment["start"] + segment["end"]) / 2
        
        frame = extract_frame_at_time(video_path, mid_time)
        
        if speaker_id not in speaker_faces:
            speaker_faces[speaker_id] = []
        speaker_faces[speaker_id].append(frame)
    
    # 3. Use VLM to describe each speaker
    speaker_descriptions = {}
    for speaker_id, faces in speaker_faces.items():
        desc = describe_person(faces[0])  # Use first clear face
        speaker_descriptions[speaker_id] = desc
    
    return speaker_descriptions
```

---

## 4. Subtitle Generation

Create SRT with accurate timing:

```python
def generate_subtitles(video_path: str) -> str:
    """Generate SRT subtitles from video."""
    
    # Transcribe with timestamps
    audio_path = extract_audio(video_path)
    
    response = client.audio.transcriptions.create(
        model="whisper-1",
        file=open(audio_path, "rb"),
        response_format="verbose_json",
        timestamp_granularities=["segment"]
    )
    
    # Convert to SRT
    srt_lines = []
    for i, segment in enumerate(response.segments, 1):
        start = format_srt_time(segment["start"])
        end = format_srt_time(segment["end"])
        text = segment["text"].strip()
        
        srt_lines.append(f"{i}")
        srt_lines.append(f"{start} --> {end}")
        srt_lines.append(text)
        srt_lines.append("")
    
    srt_content = "\n".join(srt_lines)
    
    # Save
    srt_path = video_path.rsplit(".", 1)[0] + ".srt"
    Path(srt_path).write_text(srt_content)
    
    return srt_path
```

---

## 5. Audio-Visual Alignment

Match transcript to visual content:

```python
def align_transcript_to_frames(video_path: str):
    """Create frame-by-frame transcript alignment."""
    
    # Get transcript with word-level timestamps
    transcript = transcribe_with_words(extract_audio(video_path))
    
    # Get frame timestamps
    frames = extract_frames_with_times(video_path)
    
    # Match words to nearest frame
    aligned = []
    for frame_time, frame_path in frames:
        # Find words spoken around this frame time
        nearby_words = [
            w for w in transcript.words
            if abs(w["start"] - frame_time) < 2.0  # Within 2 seconds
        ]
        
        text = " ".join(w["word"] for w in nearby_words)
        aligned.append({
            "time": frame_time,
            "frame": frame_path,
            "spoken_text": text
        })
    
    return aligned
```

---

## Quick Reference

| Task | Components |
|------|------------|
| **Extract Audio** | FFmpeg |
| **Transcribe** | Whisper |
| **Diarize** | pyannote.audio |
| **Align** | Word timestamps + frame times |
