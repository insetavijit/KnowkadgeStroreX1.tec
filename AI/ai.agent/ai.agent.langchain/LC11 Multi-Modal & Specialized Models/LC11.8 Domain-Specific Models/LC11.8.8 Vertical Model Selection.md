| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.8.8.1 Evaluation Criteria**             | Assessment                              | How to evaluate domain models                                                 | Evaluate on domain-specific benchmarks.                         |
| **LC11.8.8.2 Benchmark Testing**               | Testing                                 | Domain-specific benchmark testing                                             | Test on industry-standard benchmarks.                           |
| **LC11.8.8.3 Vendor Assessment**               | Procurement                             | Evaluating model providers                                                    | Assess vendors for compliance and quality.                      |
| **LC11.8.8.4 Build vs Buy**                    | Decision                                | When to build custom vs use existing                                          | Buy when domain overlap; build for unique needs.                |
| **LC11.8.8.5 Integration Considerations**      | Implementation                          | Integrating vertical models                                                   | Plan for API compatibility and fallbacks.                       |

# Vertical Model Selection: Choosing Specialized Models

Selecting the right model for your domain.

---

## 1. Evaluation Criteria

| Criterion | Questions to Ask |
|-----------|------------------|
| **Accuracy** | How well does it perform on domain tasks? |
| **Coverage** | Does it cover your specific sub-domain? |
| **Compliance** | Does it meet regulatory requirements? |
| **Cost** | Total cost of ownership? |
| **Latency** | Fast enough for your use case? |
| **Support** | Vendor reliability and roadmap? |

---

## 2. Domain Benchmark Testing

```python
class DomainBenchmark:
    """Benchmark models on domain-specific tasks."""
    
    def __init__(self, test_cases: list[dict]):
        self.test_cases = test_cases
    
    def evaluate_model(self, model, model_name: str) -> dict:
        results = {
            "model": model_name,
            "accuracy": [],
            "latency": [],
            "format_compliance": []
        }
        
        for case in self.test_cases:
            start = time.time()
            response = model.invoke(case["input"])
            latency = time.time() - start
            
            results["latency"].append(latency)
            results["accuracy"].append(
                self.score(response, case["expected"])
            )
            results["format_compliance"].append(
                self.check_format(response, case["format"])
            )
        
        return {
            "model": model_name,
            "avg_accuracy": sum(results["accuracy"]) / len(results["accuracy"]),
            "avg_latency_ms": sum(results["latency"]) * 1000 / len(results["latency"]),
            "format_rate": sum(results["format_compliance"]) / len(results["format_compliance"])
        }
    
    def compare_models(self, models: dict) -> pd.DataFrame:
        """Compare multiple models."""
        results = []
        for name, model in models.items():
            results.append(self.evaluate_model(model, name))
        return pd.DataFrame(results)

# Usage
benchmark = DomainBenchmark(legal_test_cases)
results = benchmark.compare_models({
    "gpt-4o": gpt4_model,
    "claude-3": claude_model,
    "legal-llama": legal_llama
})
```

---

## 3. Vendor Assessment Checklist

```python
VENDOR_ASSESSMENT = {
    "technical": {
        "model_quality": "Benchmark scores",
        "api_reliability": "Uptime SLA",
        "latency": "P50/P99 response times",
        "rate_limits": "Requests per minute",
        "context_window": "Maximum tokens"
    },
    "compliance": {
        "data_handling": "Where is data processed?",
        "certifications": "SOC2, HIPAA BAA, ISO 27001",
        "data_retention": "How long is data kept?",
        "gdpr_compliance": "Data processing agreement"
    },
    "business": {
        "pricing": "Per-token, subscription, usage",
        "support": "SLA, dedicated support",
        "roadmap": "Future development plans",
        "lock_in": "Data portability, exit strategy"
    }
}

def assess_vendor(vendor_name: str, answers: dict) -> dict:
    score = 0
    max_score = len(VENDOR_ASSESSMENT) * 3
    
    for category, criteria in VENDOR_ASSESSMENT.items():
        for criterion, description in criteria.items():
            if answers.get(f"{category}.{criterion}"):
                score += 1
    
    return {
        "vendor": vendor_name,
        "score": score,
        "max_score": max_score,
        "percentage": score / max_score * 100
    }
```

---

## 4. Build vs Buy Decision

| Factor | Build | Buy |
|--------|-------|-----|
| **Unique domain** | ✓ | |
| **Commodity problem** | | ✓ |
| **Data/IP sensitive** | ✓ | |
| **Limited resources** | | ✓ |
| **Competitive advantage** | ✓ | |
| **Time to market** | | ✓ |
| **In-house ML expertise** | ✓ | |
| **Compliance complexity** | Depends | Depends |

**Decision Framework**:
```python
def build_vs_buy_decision(factors: dict) -> str:
    build_score = 0
    buy_score = 0
    
    if factors["unique_domain"]:
        build_score += 2
    if factors["commodity_use_case"]:
        buy_score += 2
    if factors["sensitive_data"]:
        build_score += 3
    if factors["limited_ml_team"]:
        buy_score += 2
    if factors["time_critical"]:
        buy_score += 2
    if factors["competitive_advantage"]:
        build_score += 2
    
    if build_score > buy_score:
        return "BUILD: Custom model recommended"
    elif buy_score > build_score:
        return "BUY: Use vendor model"
    else:
        return "HYBRID: Use vendor model + domain fine-tuning"
```

---

## 5. Integration Considerations

```python
class VerticalModelIntegration:
    """Integrate specialized model with fallbacks."""
    
    def __init__(self):
        # Primary: Domain-specific model
        self.primary = DomainModel()
        
        # Fallback: General model
        self.fallback = ChatOpenAI(model="gpt-4o")
        
        # Cache for deterministic queries
        self.cache = RedisCache()
    
    async def invoke(self, prompt: str) -> str:
        # Check cache
        cached = await self.cache.get(hash(prompt))
        if cached:
            return cached
        
        try:
            # Try primary model
            response = await self.primary.ainvoke(prompt)
            
            # Validate response
            if self.validate(response):
                await self.cache.set(hash(prompt), response)
                return response
            else:
                raise ValidationError("Invalid response")
                
        except (TimeoutError, RateLimitError, ValidationError) as e:
            # Log and fallback
            logger.warning(f"Primary failed: {e}, using fallback")
            return await self.fallback.ainvoke(prompt)
    
    def validate(self, response: str) -> bool:
        """Domain-specific validation."""
        # Check format, terminology, etc.
        return True
```

---

## Quick Reference

| Decision | Criteria |
|----------|----------|
| **Evaluate** | Domain benchmarks + compliance |
| **Vendor** | Technical + compliance + business |
| **Build** | Unique, sensitive, competitive |
| **Buy** | Commodity, fast, limited team |
| **Integrate** | Primary + fallback + cache |
