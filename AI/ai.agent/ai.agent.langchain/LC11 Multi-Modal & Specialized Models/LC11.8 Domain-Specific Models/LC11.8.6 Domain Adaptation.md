| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.8.6.1 Adaptation Techniques**           | Methods                                 | Ways to adapt models to domains                                               | Combine prompting, RAG, and fine-tuning.                        |
| **LC11.8.6.2 RAG for Domain Knowledge**        | Retrieval                               | Adding domain knowledge via RAG                                               | RAG adds domain docs without training.                          |
| **LC11.8.6.3 Prompt Engineering**              | Prompting                               | Domain-specific prompting strategies                                          | Domain prompts shape model behavior.                            |
| **LC11.8.6.4 Continued Pre-Training**          | Training                                | Further pretraining on domain corpus                                          | Continued pretraining teaches domain language.                  |
| **LC11.8.6.5 Adaptation Pipeline**             | End-to-end                              | Complete domain adaptation workflow                                           | Combine techniques for best results.                            |

# Domain Adaptation: Making Models Domain-Aware

Adapting general models to specific domains without full retraining.

---

## 1. Adaptation Techniques Spectrum

| Technique | Effort | Effectiveness | When to Use |
|-----------|--------|---------------|-------------|
| **Prompting** | Low | Medium | Quick start |
| **RAG** | Medium | High | External knowledge |
| **Fine-Tuning** | High | Very High | Custom behavior |
| **Continued Pre-Training** | Very High | Highest | Deep domain shift |

---

## 2. RAG for Domain Knowledge

Add domain expertise without training:

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

class DomainExpert:
    def __init__(self, domain_docs_path: str):
        # Load domain documents
        docs = load_documents(domain_docs_path)
        
        # Create domain knowledge base
        self.vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=OpenAIEmbeddings()
        )
        
        self.llm = ChatOpenAI(model="gpt-4o")
    
    def answer(self, question: str) -> str:
        # Retrieve domain knowledge
        context = self.vectorstore.similarity_search(question, k=5)
        
        # Answer with domain context
        prompt = f"""
Using this domain knowledge:
{format_docs(context)}

Answer: {question}

Cite sources when applicable.
"""
        return self.llm.invoke(prompt).content
```

---

## 3. Domain Prompting

Shape behavior through prompts:

```python
# Legal domain
LEGAL_SYSTEM = """
You are a legal research assistant specializing in contract law.
- Use precise legal terminology
- Cite relevant statutes and cases
- Note jurisdictional differences
- Highlight potential issues
- Do NOT provide legal advice
"""

# Medical domain
MEDICAL_SYSTEM = """
You are a medical information assistant.
- Use clinical terminology appropriately
- Reference evidence-based medicine
- Note when evidence is limited
- Always recommend professional consultation
- Comply with HIPAA principles
"""

# Finance domain
FINANCE_SYSTEM = """
You are a financial analysis assistant.
- Use proper financial terminology
- Calculate and explain ratios
- Consider market context
- Note assumptions and limitations
- Not investment advice
"""
```

---

## 4. Continued Pre-Training

For deep domain shifts:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments

# Load base model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

# Prepare domain corpus (unlabeled text)
domain_corpus = load_texts("./legal_documents/")
tokenized = tokenize_corpus(domain_corpus, tokenizer)

# Continue pre-training
training_args = TrainingArguments(
    output_dir="./legal-llama-pretrained",
    num_train_epochs=1,  # Usually just 1 epoch
    learning_rate=5e-5,  # Lower than fine-tuning
    per_device_train_batch_size=8,
)

# Train on domain text (next token prediction)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized,
)

trainer.train()
# Then fine-tune on task-specific data
```

---

## 5. Complete Adaptation Pipeline

```python
class DomainAdaptationPipeline:
    """
    Pipeline:
    1. Start with prompting
    2. Add RAG for domain knowledge
    3. Fine-tune for specific tasks
    4. Continued pre-training for deep shift (if needed)
    """
    
    def __init__(self, domain: str):
        self.domain = domain
        
        # Level 1: Domain prompt
        self.system_prompt = load_domain_prompt(domain)
        
        # Level 2: RAG knowledge base
        self.vectorstore = build_domain_vectorstore(domain)
        
        # Level 3: Fine-tuned model (if available)
        self.fine_tuned = load_fine_tuned_model(domain)
        
        # Default LLM
        self.base_llm = ChatOpenAI(model="gpt-4o")
    
    def process(self, query: str, use_finetuned: bool = True) -> str:
        # Get domain context
        context = self.vectorstore.similarity_search(query, k=5)
        
        # Choose model
        llm = self.fine_tuned if use_finetuned and self.fine_tuned else self.base_llm
        
        # Build prompt with domain context
        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=f"Context:\n{format_docs(context)}\n\nQuery: {query}")
        ]
        
        return llm.invoke(messages).content
```

---

## Quick Reference

| Level | Technique | Data Required |
|-------|-----------|---------------|
| **1** | Prompting | None |
| **2** | RAG | Domain documents |
| **3** | Fine-tuning | Task examples |
| **4** | Pre-training | Large domain corpus |
