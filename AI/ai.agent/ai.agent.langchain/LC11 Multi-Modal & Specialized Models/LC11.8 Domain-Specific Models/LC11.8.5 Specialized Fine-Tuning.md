| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.8.5.1 Why Fine-Tune?**                  | Motivation                              | When fine-tuning beats prompting                                              | Fine-tune when prompting isn't enough.                          |
| **LC11.8.5.2 Fine-Tuning Approaches**          | Methods                                 | Full, LoRA, QLoRA fine-tuning                                                 | LoRA is efficient for most use cases.                           |
| **LC11.8.5.3 Data Preparation**                | Dataset                                 | Creating fine-tuning datasets                                                 | Quality data is critical for fine-tuning.                       |
| **LC11.8.5.4 Training Process**                | Execution                               | Running fine-tuning                                                           | Use libraries like transformers, axolotl.                       |
| **LC11.8.5.5 Evaluation**                      | Testing                                 | Measuring fine-tuning success                                                 | Test on domain-specific benchmarks.                             |

# Specialized Fine-Tuning: Domain Adaptation

Making models experts in your domain.

---

## 1. When to Fine-Tune

| Fine-Tune When | Stick with Prompting When |
|----------------|---------------------------|
| Specific output format needed | Task is well-defined |
| Domain-specific terminology | Prompt can guide adequately |
| Large-scale deployment | Limited data available |
| Latency matters (smaller model) | Rapid iteration needed |
| Proprietary knowledge required | General capabilities sufficient |

---

## 2. Fine-Tuning Approaches

**Full Fine-Tuning**:
- Update all weights
- Best quality
- Expensive (GPU memory)

**LoRA (Low-Rank Adaptation)**:
```python
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,              # Rank
    lora_alpha=32,     # Scaling
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, config)
# Only ~0.1% of parameters trained
```

**QLoRA** (Quantized LoRA):
- 4-bit quantization + LoRA
- Train 70B model on single GPU

---

## 3. Data Preparation

```python
# Format: instruction, input, output
dataset = [
    {
        "instruction": "Extract the diagnosis from this clinical note",
        "input": "Patient presents with persistent cough...",
        "output": "Primary: Acute bronchitis. Rule out: Pneumonia."
    },
    # ... more examples
]

# Minimum: 100-1000 examples
# Quality > Quantity
# Cover edge cases
```

**Dataset Best Practices**:
| Practice | Why |
|----------|-----|
| **Diverse examples** | Prevent overfitting |
| **Clean data** | Garbage in, garbage out |
| **Consistent format** | Model learns pattern |
| **Include edge cases** | Robust to real inputs |

---

## 4. Training Process

**Using Hugging Face**:
```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./models/domain-llm",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=eval_data,
)

trainer.train()
```

**Using Axolotl** (simpler config):
```yaml
# config.yml
base_model: meta-llama/Llama-2-7b-hf
model_type: LlamaForCausalLM
tokenizer_type: LlamaTokenizer

adapter: lora
lora_r: 16
lora_alpha: 32

datasets:
  - path: ./my_domain_data.jsonl
    type: alpaca

output_dir: ./domain-llama
```

---

## 5. Evaluation

```python
def evaluate_domain_model(model, test_set: list) -> dict:
    """Evaluate fine-tuned model on domain tasks."""
    results = {
        "accuracy": [],
        "format_compliance": [],
        "domain_specific": []
    }
    
    for item in test_set:
        prediction = model.generate(item["input"])
        
        # Check accuracy
        results["accuracy"].append(
            score_answer(prediction, item["expected"])
        )
        
        # Check format
        results["format_compliance"].append(
            follows_format(prediction)
        )
        
        # Domain-specific metrics
        results["domain_specific"].append(
            domain_metric(prediction, item)
        )
    
    return {k: sum(v)/len(v) for k, v in results.items()}
```

**Compare to baseline**:
| Metric | Base Model | Fine-Tuned |
|--------|------------|------------|
| Domain Accuracy | 65% | 89% |
| Format Compliance | 70% | 98% |
| Latency | 500ms | 150ms (smaller) |

---

## Quick Reference

| Approach | Params Trained | GPU Memory |
|----------|----------------|------------|
| **Full** | 100% | Very High |
| **LoRA** | ~0.1% | Medium |
| **QLoRA** | ~0.1% | Low |
