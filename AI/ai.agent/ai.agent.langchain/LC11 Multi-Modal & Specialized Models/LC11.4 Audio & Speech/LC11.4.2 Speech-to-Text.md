| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.4.2.1 Basic Transcription**             | Simple usage                            | Converting audio to text                                                      | STT converts spoken audio to text.                              |
| **LC11.4.2.2 Transcription Chain**             | LCEL pattern                            | Building audio-to-text-to-LLM pipelines                                       | Chain: Audio → Whisper → LLM → Response.                        |
| **LC11.4.2.3 Quality Factors**                 | Accuracy                                | Audio quality, accents, noise impact                                          | Clean audio = better transcription.                             |
| **LC11.4.2.4 Error Correction**                | Post-processing                         | Using LLM to fix transcription errors                                         | LLM can clean up transcription errors.                          |
| **LC11.4.2.5 Diarization**                     | Speaker separation                      | Identifying who said what                                                     | Diarization = speaker identification.                           |

# Speech-to-Text: Audio to Text Pipelines

Converting spoken audio into processable text for LLM pipelines.

---

## 1. Basic Transcription

```python
from openai import OpenAI

client = OpenAI()

def transcribe(audio_path: str) -> str:
    """Basic audio to text transcription."""
    with open(audio_path, "rb") as f:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=f
        )
    return transcript.text
```

---

## 2. Transcription Chain (Audio → LLM)

Build end-to-end audio processing:

```python
from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

def transcribe_audio(audio_path: str) -> str:
    with open(audio_path, "rb") as f:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=f
        )
    return transcript.text

audio_chain = (
    RunnableLambda(transcribe_audio)
    | ChatPromptTemplate.from_template("Summarize this transcript:\n{text}")
    | llm
    | StrOutputParser()
)

summary = audio_chain.invoke("meeting_recording.mp3")
```

---

## 3. Quality Factors

| Factor | Impact | Mitigation |
|--------|--------|------------|
| **Background Noise** | Major | Noise reduction preprocessing |
| **Accents** | Medium | Large model handles better |
| **Audio Quality** | Major | Use high bitrate recordings |
| **Multiple Speakers** | Medium | Diarization |
| **Technical Jargon** | Low | Prompt with terminology hints |

**Prompt Hint Example**:
```python
transcript = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    prompt="Technical discussion about Kubernetes, Docker, and microservices."
)
```

---

## 4. Error Correction Chain

Use LLM to clean transcription:

```python
correction_prompt = ChatPromptTemplate.from_template("""
Clean up this transcription:
- Fix obvious spelling errors
- Add proper punctuation
- Fix misheard technical terms
- Keep the original meaning

Transcript:
{transcript}

Corrected version:
""")

correction_chain = correction_prompt | llm | StrOutputParser()

raw_transcript = transcribe("audio.mp3")
cleaned = correction_chain.invoke({"transcript": raw_transcript})
```

---

## 5. Speaker Diarization

Identifying WHO said what:

```python
# pip install pyannote.audio

from pyannote.audio import Pipeline

diarization = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token="YOUR_HF_TOKEN"
)

# Get speaker segments
result = diarization("audio.wav")

for turn, _, speaker in result.itertracks(yield_label=True):
    print(f"Speaker {speaker}: {turn.start:.1f}s - {turn.end:.1f}s")

# Combine with Whisper
# Speaker 1: "Hello, welcome to the meeting."
# Speaker 2: "Thanks for having me."
```

---

## Quick Reference

| Task | Tool/Approach |
|------|---------------|
| **Transcription** | Whisper API |
| **Correction** | LLM post-processing |
| **Diarization** | pyannote.audio |
| **Noise Reduction** | noisereduce library |
