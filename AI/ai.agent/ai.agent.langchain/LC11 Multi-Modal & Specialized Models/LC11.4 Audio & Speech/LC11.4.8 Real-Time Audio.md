| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.4.8.1 Streaming STT**                   | Live transcription                      | Real-time speech to text                                                      | Stream audio to STT for live transcription.                     |
| **LC11.4.8.2 Microphone Input**                | Recording                               | Capturing live audio from microphone                                          | Use sounddevice or pyaudio for mic input.                       |
| **LC11.4.8.3 WebSocket Streaming**             | Network                                 | Streaming audio over WebSockets                                               | WebSockets for real-time audio streaming.                       |
| **LC11.4.8.4 Low Latency Considerations**      | Performance                             | Minimizing delay in live audio                                                | Balance chunk size vs latency for live audio.                   |
| **LC11.4.8.5 Live Captioning**                 | Use case                                | Real-time subtitle generation                                                 | Live captions require streaming STT + display.                  |

# Real-Time Audio: Live Speech Processing

Processing audio as it happens, not from files.

---

## 1. Streaming STT Approaches

| Approach | Latency | Quality |
|----------|---------|---------|
| **Chunked Whisper** | 5-10s | High |
| **Deepgram Streaming** | <1s | High |
| **Google Streaming** | <1s | Good |
| **Vosk (local)** | 0.5s | Good |

**Chunked Whisper** (simple, higher latency):
```python
import queue
import threading

audio_queue = queue.Queue()

def transcribe_chunks():
    buffer = []
    while True:
        chunk = audio_queue.get()
        buffer.append(chunk)
        
        # Process every 5 seconds of audio
        if sum(len(c) for c in buffer) > 16000 * 5:
            audio = np.concatenate(buffer)
            transcript = whisper_model.transcribe(audio)
            print(transcript["text"])
            buffer = []
```

---

## 2. Microphone Input

```bash
pip install sounddevice numpy
```

```python
import sounddevice as sd
import numpy as np

def record_audio(duration: int = 5, sample_rate: int = 16000) -> np.ndarray:
    """Record audio from microphone."""
    print("Recording...")
    audio = sd.rec(
        int(duration * sample_rate),
        samplerate=sample_rate,
        channels=1,
        dtype=np.float32
    )
    sd.wait()  # Wait until recording is done
    print("Done!")
    return audio.flatten()

# Stream continuously
def stream_audio(callback, sample_rate: int = 16000, chunk_size: int = 1024):
    """Stream audio to callback function."""
    with sd.InputStream(
        samplerate=sample_rate,
        channels=1,
        dtype=np.float32,
        blocksize=chunk_size,
        callback=lambda indata, frames, time, status: callback(indata.copy())
    ):
        print("Streaming... Press Ctrl+C to stop.")
        while True:
            sd.sleep(100)
```

---

## 3. WebSocket Streaming

**Server (FastAPI)**:
```python
from fastapi import FastAPI, WebSocket
import asyncio

app = FastAPI()

@app.websocket("/ws/transcribe")
async def websocket_transcribe(websocket: WebSocket):
    await websocket.accept()
    
    buffer = bytearray()
    
    try:
        while True:
            # Receive audio chunk
            chunk = await websocket.receive_bytes()
            buffer.extend(chunk)
            
            # Process when buffer is large enough
            if len(buffer) > 16000 * 2 * 5:  # 5 seconds at 16kHz
                transcript = transcribe_buffer(buffer)
                await websocket.send_text(transcript)
                buffer = bytearray()
    except:
        await websocket.close()
```

**Client**:
```python
import websockets
import asyncio

async def stream_to_server(audio_generator):
    async with websockets.connect("ws://localhost:8000/ws/transcribe") as ws:
        async def send():
            for chunk in audio_generator:
                await ws.send(chunk.tobytes())
        
        async def receive():
            while True:
                transcript = await ws.recv()
                print(f"Transcript: {transcript}")
        
        await asyncio.gather(send(), receive())
```

---

## 4. Low Latency Tips

| Setting | Impact |
|---------|--------|
| **Chunk Size** | Smaller = lower latency, more overhead |
| **Sample Rate** | 16kHz is optimal for speech |
| **Buffer Size** | Match to processing speed |
| **Model Size** | Smaller = faster (tiny, base) |

**Optimal Config for Live**:
```python
SAMPLE_RATE = 16000
CHUNK_DURATION = 0.5  # 500ms chunks
BUFFER_DURATION = 3   # 3 seconds before processing

chunk_samples = int(SAMPLE_RATE * CHUNK_DURATION)
buffer_samples = int(SAMPLE_RATE * BUFFER_DURATION)
```

---

## 5. Live Captioning Example

```python
import asyncio
from collections import deque

class LiveCaptioner:
    def __init__(self, whisper_model):
        self.model = whisper_model
        self.buffer = deque(maxlen=16000 * 10)  # 10 sec max
    
    async def process_chunk(self, audio_chunk: np.ndarray):
        self.buffer.extend(audio_chunk)
        
        # Only process if we have enough audio
        if len(self.buffer) >= 16000 * 3:  # 3 seconds
            audio = np.array(self.buffer)
            
            # Transcribe
            result = self.model.transcribe(
                audio,
                fp16=False,
                language="en"
            )
            
            # Return latest text
            return result["text"]
        
        return None

# Usage with display
captioner = LiveCaptioner(model)

async def display_captions(audio_stream):
    async for chunk in audio_stream:
        caption = await captioner.process_chunk(chunk)
        if caption:
            print(f"\r{caption}", end="", flush=True)
```

---

## Quick Reference

| Component | Tool |
|-----------|------|
| **Mic Input** | sounddevice, pyaudio |
| **Streaming STT** | Deepgram, Google, Vosk |
| **Transport** | WebSocket |
| **Local Whisper** | faster-whisper |
