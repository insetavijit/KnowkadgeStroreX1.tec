| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.4.4.1 Long Audio Handling**             | Duration                                | Processing audio longer than API limits                                       | Split long audio into chunks for processing.                    |
| **LC11.4.4.2 Chunking Strategies**             | Segmentation                            | Splitting audio at silence or fixed intervals                                 | Split at silences for natural breaks.                           |
| **LC11.4.4.3 Timestamp Generation**            | Alignment                               | Getting word/segment timestamps                                               | Request timestamps for alignment tasks.                         |
| **LC11.4.4.4 Transcript Formatting**           | Output                                  | SRT, VTT, JSON transcript formats                                             | Generate subtitles in SRT or VTT format.                        |
| **LC11.4.4.5 Podcast/Meeting Transcription**   | Use cases                               | Transcribing long-form audio content                                          | Chunk-based approach for podcasts/meetings.                     |

# Audio Transcription: Long Audio Processing

Handling audio files longer than API limits (typically 25MB/25 minutes).

---

## 1. Long Audio Challenge

OpenAI Whisper API limits:
*   **File size**: 25 MB max
*   **Duration**: ~25 minutes (depending on bitrate)

For longer content, you must chunk.

---

## 2. Chunking Strategies

**Strategy A: Fixed Duration**
```python
from pydub import AudioSegment

def chunk_audio_fixed(audio_path: str, chunk_mins: int = 10):
    """Split audio into fixed-length chunks."""
    audio = AudioSegment.from_file(audio_path)
    chunk_length_ms = chunk_mins * 60 * 1000
    
    chunks = []
    for i in range(0, len(audio), chunk_length_ms):
        chunk = audio[i:i + chunk_length_ms]
        chunks.append(chunk)
    
    return chunks
```

**Strategy B: Split on Silence (Recommended)**
```python
from pydub import AudioSegment
from pydub.silence import split_on_silence

def chunk_audio_silence(audio_path: str, 
                        min_silence_len: int = 700,
                        silence_thresh: int = -40):
    """Split audio at natural pauses."""
    audio = AudioSegment.from_file(audio_path)
    
    chunks = split_on_silence(
        audio,
        min_silence_len=min_silence_len,  # ms of silence
        silence_thresh=silence_thresh,    # dBFS
        keep_silence=500                  # keep some silence for context
    )
    
    # Merge small chunks to stay under limit
    merged = []
    current = AudioSegment.empty()
    for chunk in chunks:
        if len(current) + len(chunk) < 10 * 60 * 1000:  # 10 min
            current += chunk
        else:
            merged.append(current)
            current = chunk
    if len(current) > 0:
        merged.append(current)
    
    return merged
```

---

## 3. Timestamp Generation

```python
transcript = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    response_format="verbose_json",
    timestamp_granularities=["segment", "word"]
)

# Segment-level timestamps
for segment in transcript.segments:
    print(f"[{segment['start']:.2f}s] {segment['text']}")

# Word-level timestamps
for word in transcript.words:
    print(f"[{word['start']:.2f}s] {word['word']}")
```

---

## 4. Transcript Formatting (SRT/VTT)

**Generate SRT subtitles**:
```python
def to_srt(segments: list) -> str:
    """Convert Whisper segments to SRT format."""
    srt_lines = []
    
    for i, seg in enumerate(segments, 1):
        start = format_timestamp_srt(seg['start'])
        end = format_timestamp_srt(seg['end'])
        text = seg['text'].strip()
        
        srt_lines.append(f"{i}")
        srt_lines.append(f"{start} --> {end}")
        srt_lines.append(text)
        srt_lines.append("")
    
    return "\n".join(srt_lines)

def format_timestamp_srt(seconds: float) -> str:
    """Format seconds as SRT timestamp (00:00:00,000)."""
    hrs = int(seconds // 3600)
    mins = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hrs:02d}:{mins:02d}:{secs:02d},{millis:03d}"
```

---

## 5. Complete Long Audio Pipeline

```python
import tempfile
from pathlib import Path

def transcribe_long_audio(audio_path: str) -> str:
    """Transcribe audio of any length."""
    audio = AudioSegment.from_file(audio_path)
    
    # Check if chunking needed
    if len(audio) <= 20 * 60 * 1000:  # 20 minutes
        with open(audio_path, "rb") as f:
            return client.audio.transcriptions.create(
                model="whisper-1", file=f
            ).text
    
    # Chunk and transcribe
    chunks = chunk_audio_silence(audio_path)
    transcripts = []
    
    for i, chunk in enumerate(chunks):
        # Save chunk to temp file
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as f:
            chunk.export(f.name, format="mp3")
            with open(f.name, "rb") as audio_file:
                transcript = client.audio.transcriptions.create(
                    model="whisper-1", file=audio_file
                )
            transcripts.append(transcript.text)
            Path(f.name).unlink()  # Clean up
    
    return " ".join(transcripts)
```

---

## Quick Reference

| Audio Length | Approach |
|--------------|----------|
| **< 25 min** | Direct API call |
| **25 min - 2 hr** | Chunk on silence |
| **> 2 hr** | Batch processing with progress |
