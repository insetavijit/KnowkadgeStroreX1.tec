| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.4.6.1 Voice Interface Design**          | UX                                      | Designing conversational voice experiences                                    | Design for voice: short responses, clear structure.             |
| **LC11.4.6.2 STT → LLM → TTS Pipeline**        | End-to-end                              | Complete voice assistant architecture                                         | Full loop: Listen → Think → Speak.                              |
| **LC11.4.6.3 Conversation State**              | Memory                                  | Maintaining context across voice turns                                        | Track conversation history for context.                         |
| **LC11.4.6.4 Wake Words**                      | Activation                              | Triggering voice assistants                                                   | Wake words activate voice assistants.                           |
| **LC11.4.6.5 Voice Assistant Frameworks**      | Tools                                   | LangChain + voice frameworks integration                                      | Use Rasa, Voiceflow, or custom solutions.                       |

# Voice Assistants: Spoken AI Interfaces

Building conversational AI with voice input and output.

---

## 1. Voice Interface Design

Voice UX differs from text:
*   **Short responses**: Users can't "scan" audio
*   **Clear structure**: "First... Second... Finally..."
*   **Confirmation**: Repeat key information back
*   **Error recovery**: "I didn't understand. Did you mean..."

```python
voice_prompt = """
You are a voice assistant. Keep responses:
- Under 3 sentences unless asked for detail
- Use natural conversational language
- Avoid lists (say "You have three options: ..." instead)
- End with a question when appropriate
"""
```

---

## 2. Complete Voice Pipeline

```python
from openai import OpenAI

client = OpenAI()

class VoiceAssistant:
    def __init__(self):
        self.llm = ChatOpenAI()
        self.history = []
    
    def listen(self, audio_path: str) -> str:
        """STT: Convert speech to text."""
        with open(audio_path, "rb") as f:
            transcript = client.audio.transcriptions.create(
                model="whisper-1", file=f
            )
        return transcript.text
    
    def think(self, user_input: str) -> str:
        """LLM: Generate response."""
        self.history.append(HumanMessage(content=user_input))
        
        response = self.llm.invoke([
            SystemMessage(content="You are a helpful voice assistant."),
            *self.history
        ])
        
        self.history.append(response)
        return response.content
    
    def speak(self, text: str) -> bytes:
        """TTS: Convert text to speech."""
        response = client.audio.speech.create(
            model="tts-1",
            voice="nova",
            input=text
        )
        return response.content
    
    def process(self, audio_path: str) -> bytes:
        """Full voice assistant loop."""
        text = self.listen(audio_path)
        response = self.think(text)
        audio = self.speak(response)
        return audio
```

---

## 3. Conversation State Management

```python
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Store sessions
session_histories = {}

def get_session_history(session_id: str):
    if session_id not in session_histories:
        session_histories[session_id] = InMemoryChatMessageHistory()
    return session_histories[session_id]

voice_chain_with_history = RunnableWithMessageHistory(
    voice_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# Each user gets their own conversation history
response = voice_chain_with_history.invoke(
    {"input": user_speech},
    config={"configurable": {"session_id": "user_123"}}
)
```

---

## 4. Wake Word Integration

Use libraries like Porcupine for wake word detection:

```python
# pip install pvporcupine

import pvporcupine

porcupine = pvporcupine.create(
    access_key="...",
    keywords=["jarvis", "hey assistant"]
)

def listen_for_wake_word(audio_stream):
    """Listen for wake word before processing."""
    while True:
        pcm = audio_stream.read(porcupine.frame_length)
        keyword_index = porcupine.process(pcm)
        
        if keyword_index >= 0:
            print("Wake word detected!")
            return True
```

---

## 5. Framework Integration

| Framework | Best For |
|-----------|----------|
| **Custom (LangChain)** | Flexibility, custom logic |
| **Voiceflow** | Visual design, enterprise |
| **Rasa** | Open source, on-premise |
| **Amazon Lex** | AWS integration |

**LangChain + FastAPI Voice Endpoint**:
```python
from fastapi import FastAPI, UploadFile

app = FastAPI()
assistant = VoiceAssistant()

@app.post("/voice")
async def voice_endpoint(audio: UploadFile):
    # Save uploaded audio
    audio_path = save_upload(audio)
    
    # Process and return audio response
    response_audio = assistant.process(audio_path)
    
    return Response(content=response_audio, media_type="audio/mpeg")
```

---

## Quick Reference

| Component | Technology |
|-----------|------------|
| **STT** | Whisper |
| **LLM** | GPT-4, Claude |
| **TTS** | OpenAI TTS, ElevenLabs |
| **Wake Word** | Porcupine, Snowboy |
| **Framework** | LangChain + FastAPI |
