| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.7.8.1 CodeLlama**                       | Meta                                    | Meta's code-specialized Llama model                                           | CodeLlama is Llama fine-tuned for code.                         |
| **LC11.7.8.2 StarCoder**                       | BigCode                                 | Open-source code model                                                        | StarCoder is trained on 80+ languages.                          |
| **LC11.7.8.3 DeepSeek Coder**                  | DeepSeek                                | High-performance code model                                                   | DeepSeek Coder rivals GPT-4 for code.                           |
| **LC11.7.8.4 Qwen-Coder**                      | Alibaba                                 | Alibaba's code model                                                          | Qwen-Coder performs well on benchmarks.                         |
| **LC11.7.8.5 Model Selection**                 | Choosing                                | Picking the right code model                                                  | Match model to task and constraints.                            |

# Code-Specific Models: Specialized for Programming

Models trained specifically on code outperform general LLMs.

---

## 1. CodeLlama

Meta's code-specialized Llama:

```python
from langchain_ollama import ChatOllama

# Run locally with Ollama
code_llm = ChatOllama(model="codellama:13b")

# Or with Hugging Face
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("codellama/CodeLlama-13b-Python-hf")
```

**Variants**:
| Model | Size | Specialty |
|-------|------|-----------|
| **CodeLlama** | 7B/13B/34B | General code |
| **CodeLlama-Python** | 7B/13B/34B | Python focused |
| **CodeLlama-Instruct** | 7B/13B/34B | Instruction-following |

---

## 2. StarCoder

BigCode project's open model:

```python
# Ollama
llm = ChatOllama(model="starcoder2:15b")

# Hugging Face
from transformers import pipeline
generator = pipeline("text-generation", model="bigcode/starcoder2-15b")
```

**Features**:
- Trained on 80+ languages
- Fill-in-the-middle support
- 8K context window
- Permissive license

---

## 3. DeepSeek Coder

High-performance code model:

```python
# API access
from openai import OpenAI

client = OpenAI(
    api_key="your-key",
    base_url="https://api.deepseek.com"
)

response = client.chat.completions.create(
    model="deepseek-coder",
    messages=[{"role": "user", "content": "Write a binary search function"}]
)

# Local via Ollama
llm = ChatOllama(model="deepseek-coder:33b")
```

---

## 4. Qwen-Coder

Alibaba's code model:

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(model="qwen2.5-coder:32b")
```

**Strengths**:
- Excellent on HumanEval benchmark
- Strong multi-language support
- Good at code explanation

---

## 5. Model Selection Guide

| Use Case | Recommended Model |
|----------|-------------------|
| **Local, small** | CodeLlama 7B |
| **Local, quality** | DeepSeek 33B, Qwen 32B |
| **Cloud API** | GPT-4o, Claude 3.5 |
| **FIM/Completion** | StarCoder2 |
| **Python specific** | CodeLlama-Python |

**Comparison** (HumanEval pass@1):
| Model | Score |
|-------|-------|
| GPT-4o | ~90% |
| DeepSeek Coder 33B | ~76% |
| CodeLlama 34B | ~53% |
| StarCoder2 15B | ~46% |

**Selection Factors**:
1. **Local vs Cloud**: Privacy needs?
2. **Size constraints**: GPU memory?
3. **Language**: Python-heavy or multi-language?
4. **Task**: Generation vs completion vs review?
5. **License**: Commercial use requirements?

---

## LangChain Integration

```python
from langchain_core.runnables import RunnableLambda

def get_code_model(task: str):
    """Select model based on task."""
    if task == "completion":
        return ChatOllama(model="starcoder2:15b")
    elif task == "generation":
        return ChatOllama(model="deepseek-coder:33b")
    elif task == "review":
        return ChatOpenAI(model="gpt-4o")  # Best reasoning
    else:
        return ChatOllama(model="codellama:13b")

# Dynamic selection
code_chain = (
    RunnableLambda(lambda x: (x, get_code_model(x["task"])))
    | RunnableLambda(lambda args: args[1].invoke(args[0]["prompt"]))
)
```

---

## Quick Reference

| Model | Provider | Best For |
|-------|----------|----------|
| **CodeLlama** | Meta | Local Python |
| **StarCoder2** | BigCode | FIM, open-source |
| **DeepSeek** | DeepSeek | Quality/cost ratio |
| **Qwen-Coder** | Alibaba | Benchmarks |
| **GPT-4o** | OpenAI | Overall best |
