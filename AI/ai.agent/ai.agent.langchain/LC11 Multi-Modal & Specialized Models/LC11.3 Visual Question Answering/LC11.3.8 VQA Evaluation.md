| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.3.8.1 VQA Accuracy**                    | Correctness                             | Measuring if answers are factually correct                                    | VQA accuracy = correct answers / total questions.               |
| **LC11.3.8.2 Benchmark Datasets**              | Standards                               | VQAv2, TextVQA, DocVQA benchmarks                                             | Use standard benchmarks for comparison.                         |
| **LC11.3.8.3 Human Evaluation**                | Quality                                 | Manual assessment of VQA quality                                              | Human eval for nuanced quality assessment.                      |
| **LC11.3.8.4 Error Analysis**                  | Debugging                               | Categorizing and understanding VQA failures                                   | Analyze errors by category to improve.                          |
| **LC11.3.8.5 Production Metrics**              | Operations                              | Latency, cost, throughput for VQA                                             | Track latency, cost, and throughput in production.              |

# VQA Evaluation: Measuring Visual QA Quality

How do you know if your VQA system is working well?

---

## 1. VQA Accuracy

**Basic Metric**: Does the answer match the ground truth?

```python
def vqa_accuracy(predictions: list[str], ground_truths: list[str]) -> float:
    """Calculate exact match accuracy."""
    correct = sum(1 for p, g in zip(predictions, ground_truths) if p.lower().strip() == g.lower().strip())
    return correct / len(predictions)
```

**Soft Accuracy** (allowing partial matches):
```python
def soft_accuracy(prediction: str, ground_truths: list[str]) -> float:
    """VQA-style soft accuracy (matches if 3+ annotators agree)."""
    matches = sum(1 for gt in ground_truths if prediction.lower() == gt.lower())
    return min(1.0, matches / 3)
```

---

## 2. Benchmark Datasets

| Dataset | Focus | Size |
|---------|-------|------|
| **VQAv2** | Natural images, general | 1.1M questions |
| **TextVQA** | Reading text in images | 45K questions |
| **DocVQA** | Document images | 50K questions |
| **ChartQA** | Chart understanding | 32K questions |
| **OKVQA** | External knowledge needed | 14K questions |

**Using Benchmarks**:
```python
# Load TextVQA sample
from datasets import load_dataset
textvqa = load_dataset("textvqa", split="validation[:100]")

# Evaluate your model
results = []
for sample in textvqa:
    prediction = vqa_chain.invoke({
        "image_url": sample["image"],
        "question": sample["question"]
    })
    results.append({
        "prediction": prediction,
        "ground_truth": sample["answers"]
    })
```

---

## 3. Human Evaluation

For nuanced quality assessment:

```python
EVAL_CRITERIA = """
Rate the answer on a 1-5 scale:
1 = Completely wrong or irrelevant
2 = Partially related but incorrect
3 = Correct but incomplete
4 = Correct and complete
5 = Correct, complete, and well-explained
"""

def create_eval_task(image_url: str, question: str, answer: str) -> dict:
    return {
        "image_url": image_url,
        "question": question,
        "model_answer": answer,
        "criteria": EVAL_CRITERIA,
        "rating": None,  # Filled by human
        "comments": None
    }
```

---

## 4. Error Analysis

Categorize failures to improve:

| Error Type | Example | Fix |
|------------|---------|-----|
| **Hallucination** | Invents objects not present | Better grounding prompts |
| **Counting Error** | "3 people" when 5 visible | Ask to count step-by-step |
| **OCR Error** | Misreads "2024" as "2034" | Specialized OCR + VLM |
| **Reasoning Error** | Logic mistake | Chain-of-thought prompting |
| **Refusal** | "I cannot see the image" | Check image format |

```python
def categorize_error(prediction: str, ground_truth: str, image: str) -> str:
    """Automatically categorize VQA errors."""
    if "cannot see" in prediction.lower() or "no image" in prediction.lower():
        return "image_access_error"
    elif prediction.isdigit() and ground_truth.isdigit():
        if int(prediction) != int(ground_truth):
            return "counting_error"
    # ... more categories
    return "other"
```

---

## 5. Production Metrics

Track operational performance:

```python
import time
from dataclasses import dataclass

@dataclass
class VQAMetrics:
    query_count: int = 0
    total_latency: float = 0.0
    total_tokens: int = 0
    error_count: int = 0
    
    @property
    def avg_latency(self) -> float:
        return self.total_latency / max(1, self.query_count)
    
    @property
    def error_rate(self) -> float:
        return self.error_count / max(1, self.query_count)

metrics = VQAMetrics()

def tracked_vqa(image_url: str, question: str) -> str:
    start = time.time()
    try:
        result = vqa_chain.invoke({"image_url": image_url, "question": question})
        metrics.query_count += 1
        metrics.total_latency += time.time() - start
        return result
    except Exception as e:
        metrics.error_count += 1
        raise
```

---

## Quick Reference

| Metric | What It Measures |
|--------|------------------|
| **Accuracy** | Correctness |
| **Latency** | Response time |
| **Token Usage** | Cost indicator |
| **Error Rate** | Reliability |
| **Human Rating** | Overall quality |
