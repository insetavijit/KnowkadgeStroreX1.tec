| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.3.2.1 Cross-Modal Embeddings**          | Representation                          | CLIP embeddings for both text and images                                      | CLIP maps text and images to same space.                        |
| **LC11.3.2.2 Text-to-Image Search**            | Finding images                          | Using text query to find relevant images                                      | Embed query text, find nearest image vectors.                   |
| **LC11.3.2.3 Image-to-Text Search**            | Finding text                            | Using image to find relevant documents                                        | Embed image, find nearest text vectors.                         |
| **LC11.3.2.4 CLIP Integration**                | Setup                                   | Using CLIP embeddings in LangChain                                            | Use OpenCLIP or CLIP models for embeddings.                     |
| **LC11.3.2.5 Multi-Modal Vector Store**        | Storage                                 | Storing both text and image embeddings                                        | Store mixed modality vectors in same store.                     |

# Image-Text Retrieval: Cross-Modal Search

Find images with text queries, or find text with image queries.

---

## 1. Cross-Modal Embeddings

CLIP (Contrastive Language-Image Pre-training) creates embeddings where:
*   Text and images share the same vector space
*   Similar concepts have similar vectors

```
"a photo of a dog" ←→ [image of dog]
     Text Vector    ~  Image Vector
```

---

## 2. Text-to-Image Search

Find images that match a text query:

```python
from langchain_experimental.open_clip import OpenCLIPEmbeddings

# Initialize CLIP
embeddings = OpenCLIPEmbeddings(model_name="ViT-B-32", checkpoint="openai")

# Embed text query
query_vector = embeddings.embed_query("a sunset over the ocean")

# Search image vector store
results = image_vectorstore.similarity_search_by_vector(query_vector, k=5)
```

---

## 3. Image-to-Text Search

Find documents relevant to an image:

```python
# Embed the image
image_vector = embeddings.embed_image("./photo.jpg")

# Search text vector store
results = text_vectorstore.similarity_search_by_vector(image_vector, k=5)
```

**Use Case**: "Find all documentation related to this screenshot"

---

## 4. CLIP Integration

**Install**:
```bash
pip install langchain-experimental open-clip-torch
```

**Usage**:
```python
from langchain_experimental.open_clip import OpenCLIPEmbeddings

# Multiple model options
clip = OpenCLIPEmbeddings(
    model_name="ViT-B-32",      # or "ViT-L-14", "ViT-H-14"
    checkpoint="openai"          # or "laion2b_s34b_b79k"
)

# Embed single image
image_embedding = clip.embed_image("path/to/image.jpg")

# Embed multiple images
image_embeddings = clip.embed_image_list(["img1.jpg", "img2.jpg"])

# Embed text
text_embedding = clip.embed_query("a cute puppy")
```

---

## 5. Multi-Modal Vector Store

Store both text and image embeddings together:

```python
from langchain_chroma import Chroma

# Create store with CLIP embeddings
vectorstore = Chroma(
    collection_name="multimodal",
    embedding_function=clip,
    persist_directory="./mm_store"
)

# Add images
image_docs = [
    Document(page_content="", metadata={"path": path, "type": "image"})
    for path in image_paths
]
vectorstore.add_documents(image_docs)

# Add text documents
vectorstore.add_documents(text_docs)

# Search with text → finds both text and images
results = vectorstore.similarity_search("dog playing fetch")
```

---

## Quick Reference

| Task | Method |
|------|--------|
| **Text → Image** | `embed_query()` → image store |
| **Image → Text** | `embed_image()` → text store |
| **Mixed Store** | Single CLIP-indexed store |
| **Model** | OpenCLIP, CLIP, SigLIP |
