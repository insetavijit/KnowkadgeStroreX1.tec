| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.3.3.1 What is Visual Grounding?**       | Definition                              | Locating image regions from text descriptions                                 | Visual grounding = "where is X in this image?"                  |
| **LC11.3.3.2 Region References**               | Text localization                       | Getting textual location descriptions                                         | VLMs describe locations in natural language.                    |
| **LC11.3.3.3 Bounding Box Extraction**         | Coordinates                             | Getting x,y coordinates from VLMs                                             | Ask for approximate bounding box coordinates.                   |
| **LC11.3.3.4 Grounding DINO**                  | Specialized model                       | Open-vocabulary detection with text prompts                                   | Grounding DINO finds objects from text descriptions.            |
| **LC11.3.3.5 SAM Integration**                 | Segmentation                            | Segment Anything Model for precise masks                                      | Use SAM for pixel-level segmentation.                           |

# Visual Grounding: Locating Objects from Descriptions

"Where in this image is the red cat?"

---

## 1. What is Visual Grounding?

**Input**: Image + Text description of an object/region
**Output**: Location (coordinates, bounding box, or mask)

Unlike object detection (finds ALL objects), grounding finds SPECIFIC described elements.

---

## 2. VLM Region References

VLMs can describe locations in natural language:

```python
message = HumanMessage(content=[
    {"type": "image_url", "image_url": {"url": image_url}},
    {"type": "text", "text": "Where is the coffee mug located in this image?"}
])

response = llm.invoke([message])
# "The coffee mug is in the bottom-right corner of the image, on the wooden desk."
```

---

## 3. Approximate Bounding Boxes

Ask VLMs for coordinates (approximate):

```python
prompt = """
Locate the "red car" in this image.
Return the bounding box as: [x_min, y_min, x_max, y_max]
Where values are percentages (0-100) of image dimensions.
"""

# Response: "The red car is approximately at [45, 30, 85, 70]"
```

**Warning**: VLM coordinates are approximate (~±10% accuracy).

---

## 4. Grounding DINO

For precise detection with text prompts:

```python
# pip install groundingdino-py

from groundingdino.util.inference import load_model, predict

model = load_model("groundingdino_swint_ogc.pth")

boxes, logits, phrases = predict(
    model=model,
    image=image,
    caption="red car . person with glasses . stop sign",
    box_threshold=0.35,
    text_threshold=0.25
)
```

**Output**: Precise bounding boxes for each described object.

---

## 5. SAM (Segment Anything) Integration

For pixel-level segmentation:

```python
from segment_anything import SamPredictor, sam_model_registry

# Load SAM
sam = sam_model_registry["vit_h"](checkpoint="sam_vit_h.pth")
predictor = SamPredictor(sam)

# Set image
predictor.set_image(image)

# Get mask from bounding box (from Grounding DINO)
masks, scores, _ = predictor.predict(
    box=bounding_box,
    multimask_output=False
)

# masks[0] is the segmentation mask
```

**Combined Pipeline**:
```
Text Query → Grounding DINO (box) → SAM (mask) → Precise Segment
```

---

## Quick Reference

| Task | Tool |
|------|------|
| **Natural Language Location** | VLM |
| **Approximate Box** | VLM with coordinates prompt |
| **Precise Box** | Grounding DINO |
| **Pixel Mask** | SAM |
| **End-to-End** | Grounded-SAM |
