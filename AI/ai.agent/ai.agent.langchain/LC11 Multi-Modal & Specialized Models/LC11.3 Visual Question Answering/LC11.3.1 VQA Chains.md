| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.3.1.1 What is VQA?**                    | Definition                              | Answering questions about visual content                                      | VQA = answering natural language questions about images.        |
| **LC11.3.1.2 Basic VQA Chain**                 | Implementation                          | Image + Question → Answer pipeline                                            | Combine image and question in HumanMessage.                     |
| **LC11.3.1.3 Question Types**                  | Categories                              | Factual, counting, comparison, reasoning                                      | VQA handles counting, comparison, and reasoning.                |
| **LC11.3.1.4 Context Injection**               | Enhancement                             | Adding background knowledge to VQA                                            | Inject context for better answers.                              |
| **LC11.3.1.5 VQA with Memory**                 | Conversational                          | Follow-up questions about same image                                          | Maintain conversational context for multi-turn VQA.             |

# VQA Chains: Answering Questions About Images

Visual Question Answering (VQA) combines vision and language understanding.

---

## 1. What is VQA?

**Input**: Image + Natural language question
**Output**: Natural language answer

Examples:
*   "How many people are in this photo?" → "Three"
*   "What color is the car?" → "Red"
*   "Is the person wearing glasses?" → "Yes"

---

## 2. Basic VQA Chain

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

llm = ChatOpenAI(model="gpt-4o")

def build_vqa_message(inputs: dict):
    return [HumanMessage(content=[
        {"type": "image_url", "image_url": {"url": inputs["image_url"]}},
        {"type": "text", "text": inputs["question"]}
    ])]

vqa_chain = (
    RunnableLambda(build_vqa_message)
    | llm
    | StrOutputParser()
)

# Usage
answer = vqa_chain.invoke({
    "image_url": "https://example.com/photo.jpg",
    "question": "What is the main subject of this image?"
})
```

---

## 3. Question Types

| Type | Example | Challenge |
|------|---------|-----------|
| **Factual** | "What brand is the laptop?" | Recognition |
| **Counting** | "How many chairs?" | Object detection |
| **Comparison** | "Which is taller, the tree or building?" | Spatial reasoning |
| **Reasoning** | "Is it a sunny day?" (infer from shadows) | Inference |
| **Reading** | "What does the sign say?" | OCR |

---

## 4. Context Injection

Add background knowledge for better answers:

```python
def build_vqa_with_context(inputs: dict):
    context = inputs.get("context", "")
    
    prompt = f"""
Answer the question about this image.

Context: {context}

Question: {inputs["question"]}
"""
    return [HumanMessage(content=[
        {"type": "image_url", "image_url": {"url": inputs["image_url"]}},
        {"type": "text", "text": prompt}
    ])]

# Usage: Provide product info when asking about products
answer = vqa_chain.invoke({
    "image_url": product_url,
    "question": "Is this in stock?",
    "context": "This is a Nike Air Max shoe from our Spring 2024 collection."
})
```

---

## 5. VQA with Memory (Multi-Turn)

For follow-up questions about the same image:

```python
from langchain_core.messages import AIMessage

messages = []

def ask_about_image(image_url: str, question: str) -> str:
    # Add image only on first message
    if not messages:
        messages.append(HumanMessage(content=[
            {"type": "image_url", "image_url": {"url": image_url}},
            {"type": "text", "text": question}
        ]))
    else:
        messages.append(HumanMessage(content=question))
    
    response = llm.invoke(messages)
    messages.append(AIMessage(content=response.content))
    
    return response.content

# Conversation
ask_about_image(url, "What's in this image?")        # "A red car..."
ask_about_image(url, "What brand is it?")            # "It appears to be a Tesla..."
ask_about_image(url, "How many people are inside?")  # "I can see two..."
```

---

## Quick Reference

| Pattern | Use Case |
|---------|----------|
| **Single VQA** | One-off questions |
| **Context VQA** | Domain-specific knowledge |
| **Multi-Turn** | Conversational analysis |
| **Batch VQA** | Same question, many images |
