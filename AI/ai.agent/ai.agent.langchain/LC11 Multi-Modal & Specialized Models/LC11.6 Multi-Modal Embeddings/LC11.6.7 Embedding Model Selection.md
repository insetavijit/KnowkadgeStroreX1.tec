| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.6.7.1 Model Comparison**                | Evaluation                              | Comparing CLIP variants and alternatives                                      | Compare models by quality, speed, and dimensions.               |
| **LC11.6.7.2 Quality vs Speed**                | Trade-offs                              | Larger models vs inference time                                               | ViT-B-32 is fast; ViT-L-14 is accurate.                         |
| **LC11.6.7.3 Open Source Options**             | Alternatives                            | OpenCLIP, SigLIP, BLIP                                                        | SigLIP often outperforms CLIP.                                  |
| **LC11.6.7.4 Task-Specific Selection**         | Use case                                | Choosing model for specific task                                              | Match model to task requirements.                               |
| **LC11.6.7.5 Benchmarking**                    | Testing                                 | Evaluating models on your data                                                | Always benchmark on your specific domain.                       |

# Embedding Model Selection: Choosing the Right Model

Different models for different needs.

---

## 1. CLIP Variant Comparison

| Model | Dimensions | Speed | ImageNet | MSCOCO |
|-------|------------|-------|----------|--------|
| **ViT-B-32** | 512 | âš¡âš¡âš¡ | 62.9% | - |
| **ViT-B-16** | 512 | âš¡âš¡ | 68.3% | - |
| **ViT-L-14** | 768 | âš¡ | 75.3% | - |
| **ViT-H-14** | 1024 | ðŸ¢ | 78.0% | - |

---

## 2. Quality vs Speed Trade-offs

```python
# Fast but lower quality
clip_fast = OpenCLIPEmbeddings(model_name="ViT-B-32", checkpoint="openai")

# Balanced
clip_balanced = OpenCLIPEmbeddings(model_name="ViT-B-16", checkpoint="openai")

# High quality, slower
clip_quality = OpenCLIPEmbeddings(model_name="ViT-L-14", checkpoint="openai")

# Highest quality, slowest
clip_best = OpenCLIPEmbeddings(model_name="ViT-H-14", checkpoint="laion2b")
```

| Use Case | Recommended |
|----------|-------------|
| **Real-time search** | ViT-B-32 |
| **Balanced needs** | ViT-B-16 |
| **Quality critical** | ViT-L-14 |
| **Research/best quality** | ViT-H-14 |

---

## 3. Alternative Models

**SigLIP** (Google) - Often better than CLIP:
```python
# With transformers
from transformers import AutoModel
model = AutoModel.from_pretrained("google/siglip-base-patch16-224")
```

**BLIP-2** (Salesforce) - Better for captioning:
```python
from transformers import Blip2Model
model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b")
```

**Comparison**:
| Model | Strength | Weakness |
|-------|----------|----------|
| **CLIP** | Balanced, well-supported | Older architecture |
| **SigLIP** | Better zero-shot | Less documentation |
| **BLIP** | Strong captioning | Heavier model |
| **OpenCLIP** | Many checkpoints | Need to choose carefully |

---

## 4. Task-Specific Selection

| Task | Best Model |
|------|------------|
| **General image search** | OpenCLIP ViT-L-14 |
| **Real-time matching** | CLIP ViT-B-32 |
| **Caption generation** | BLIP-2 |
| **Zero-shot classification** | SigLIP |
| **E-commerce products** | ViT-L-14 + fine-tuned |

---

## 5. Benchmarking Your Domain

Test on YOUR data:

```python
def benchmark_model(model, test_data: list[dict]) -> dict:
    """
    Test model on domain-specific pairs.
    test_data: [{"image": path, "positive": text, "negatives": [texts]}]
    """
    
    correct = 0
    
    for item in test_data:
        img_emb = model.embed_image(item["image"])
        pos_score = cosine_sim(img_emb, model.embed_query(item["positive"]))
        neg_scores = [
            cosine_sim(img_emb, model.embed_query(n)) 
            for n in item["negatives"]
        ]
        
        if pos_score > max(neg_scores):
            correct += 1
    
    return {
        "accuracy": correct / len(test_data),
        "model": model.model_name
    }

# Compare models
models = [clip_fast, clip_balanced, clip_quality]
results = [benchmark_model(m, domain_test_set) for m in models]
```

---

## Quick Reference

| Priority | Model |
|----------|-------|
| **Speed** | ViT-B-32 |
| **Balance** | ViT-B-16 or ViT-L-14 |
| **Quality** | ViT-H-14 + LAION checkpoint |
| **Always** | Benchmark on your data |
