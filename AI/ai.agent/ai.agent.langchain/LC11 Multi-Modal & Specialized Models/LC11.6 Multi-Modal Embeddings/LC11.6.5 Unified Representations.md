| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.6.5.1 Shared Embedding Space**          | Concept                                 | Text and images in same vector space                                          | CLIP creates unified space for all modalities.                  |
| **LC11.6.5.2 Alignment Quality**               | Evaluation                              | How well modalities align                                                     | Better alignment = better cross-modal search.                   |
| **LC11.6.5.3 Adding New Modalities**           | Extension                               | Extending to audio, video, etc.                                               | New modalities can be aligned to existing space.                |
| **LC11.6.5.4 Fine-Tuning for Domain**          | Customization                           | Domain-specific embedding alignment                                           | Fine-tune CLIP for domain-specific alignment.                   |
| **LC11.6.5.5 Limitations**                     | Caveats                                 | When unified representations fail                                             | CLIP struggles with abstract concepts.                          |

# Unified Representations: One Space for All

All modalities as vectors in the same space.

---

## 1. The Shared Space Concept

CLIP training aligns text and images:

```
Text: "a sunny beach"     â†’  [0.3, 0.7, 0.1, ...]
Image: ðŸ–ï¸ (beach photo)   â†’  [0.3, 0.6, 0.2, ...]
                              â†‘ Similar vectors
```

This enables:
- Text can find images
- Images can find text
- Distance = semantic difference

---

## 2. Alignment Quality

Not all alignments are equal:

```python
def test_alignment_quality(clip, test_pairs: list[tuple]):
    """Evaluate how well image-text pairs align."""
    
    correct = 0
    total = len(test_pairs)
    
    for image, positive_text, negative_texts in test_pairs:
        img_emb = clip.embed_image(image)
        pos_emb = clip.embed_query(positive_text)
        neg_embs = [clip.embed_query(t) for t in negative_texts]
        
        pos_sim = cosine_similarity(img_emb, pos_emb)
        neg_sims = [cosine_similarity(img_emb, e) for e in neg_embs]
        
        # Correct if positive > all negatives
        if pos_sim > max(neg_sims):
            correct += 1
    
    return correct / total

# Good alignment: > 80% accuracy on test pairs
```

---

## 3. Adding New Modalities

Extend to audio, 3D, etc:

**ImageBind** (Meta) aligns 6 modalities:
- Text, Image, Audio, Video, Depth, Thermal

```python
# Conceptual example
from imagebind import ImageBindModel

model = ImageBindModel.from_pretrained()

# All return vectors in same space
text_emb = model.embed_text("a dog barking")
image_emb = model.embed_image("/dog.jpg")
audio_emb = model.embed_audio("/bark.wav")

# Can compare any pair
text_audio_sim = cosine_similarity(text_emb, audio_emb)
```

---

## 4. Domain Fine-Tuning

Improve alignment for specific domains:

```python
# Example: Medical imaging + reports
from sentence_transformers import SentenceTransformer

# Start with CLIP
model = SentenceTransformer('clip-ViT-B-32')

# Fine-tune with domain pairs
train_examples = [
    InputExample(texts=["chest x-ray showing pneumonia"], images=["xray1.jpg"]),
    InputExample(texts=["normal lung appearance"], images=["xray2.jpg"]),
    # ... more medical image-text pairs
]

model.fit(
    train_objectives=[(train_dataloader, contrastive_loss)],
    epochs=10
)
```

---

## 5. Limitations

CLIP unified space struggles with:

| Limitation | Example |
|------------|---------|
| **Abstract concepts** | "Justice", "Democracy" |
| **Negation** | "no cat" vs "cat" often similar |
| **Spatial relations** | "cat on left, dog on right" |
| **Counting** | "three apples" vs "five apples" |
| **Rare concepts** | Domain-specific jargon |

**Mitigation**:
- Use VLMs for complex reasoning
- Fine-tune for domain
- Combine with structured metadata

---

## Quick Reference

| Aspect | Details |
|--------|---------|
| **Dimension** | 512-1024 depending on model |
| **Normalization** | Usually L2 normalized |
| **Metric** | Cosine similarity |
| **Modalities** | Text + Image (CLIP); 6+ (ImageBind) |
