| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.6.1.1 What is CLIP?**                   | Background                              | Contrastive Language-Image Pre-training                                       | CLIP maps text and images to shared vector space.               |
| **LC11.6.1.2 OpenCLIP in LangChain**           | Integration                             | Using OpenCLIP embeddings in LangChain                                        | langchain-experimental provides OpenCLIP.                       |
| **LC11.6.1.3 Image Embedding**                 | Visual vectors                          | Converting images to embeddings                                               | Embed images with embed_image().                                |
| **LC11.6.1.4 Text Embedding**                  | Text vectors                            | Converting text to CLIP-compatible embeddings                                 | Embed text with embed_query().                                  |
| **LC11.6.1.5 Model Variants**                  | Options                                 | ViT-B-32, ViT-L-14, ViT-H-14 variants                                         | Larger models = better quality, slower.                         |

# CLIP Embeddings: Bridging Text and Images

CLIP creates embeddings where related text and images are close together.

---

## 1. What is CLIP?

CLIP = Contrastive Language-Image Pre-training

**Key Idea**: Train on 400M image-text pairs so that:
- Matching pairs ‚Üí similar vectors
- Non-matching pairs ‚Üí different vectors

```
"a photo of a cat" ‚Üí [0.1, 0.5, ...] ‚Üê similar ‚Üí [0.1, 0.4, ...] ‚Üê üê±
"sunset at beach"  ‚Üí [0.7, 0.2, ...] ‚Üê similar ‚Üí [0.6, 0.3, ...] ‚Üê üåÖ
```

---

## 2. LangChain Integration

```bash
pip install langchain-experimental open-clip-torch
```

```python
from langchain_experimental.open_clip import OpenCLIPEmbeddings

# Initialize
clip = OpenCLIPEmbeddings(
    model_name="ViT-B-32",    # Model architecture
    checkpoint="openai"       # Weights source
)
```

---

## 3. Image Embedding

```python
# Single image
image_embedding = clip.embed_image("/path/to/image.jpg")
# Returns: list[float] - 512 or 768 dimensions

# Multiple images
image_embeddings = clip.embed_image([
    "/path/to/image1.jpg",
    "/path/to/image2.jpg",
    "/path/to/image3.jpg"
])
# Returns: list[list[float]]

# From URL
image_embedding = clip.embed_image("https://example.com/photo.jpg")
```

---

## 4. Text Embedding

```python
# Single text
text_embedding = clip.embed_query("a golden retriever puppy")
# Returns: list[float]

# Multiple texts
text_embeddings = clip.embed_documents([
    "a golden retriever puppy",
    "sunset over the ocean",
    "person riding a bicycle"
])
# Returns: list[list[float]]
```

---

## 5. Model Variants

| Model | Dimensions | Speed | Quality |
|-------|------------|-------|---------|
| **ViT-B-32** | 512 | Fast | Good |
| **ViT-B-16** | 512 | Medium | Better |
| **ViT-L-14** | 768 | Slow | Very Good |
| **ViT-H-14** | 1024 | Slowest | Best |

**Checkpoint Options**:
```python
# OpenAI original weights
clip = OpenCLIPEmbeddings(model_name="ViT-L-14", checkpoint="openai")

# LAION fine-tuned (often better)
clip = OpenCLIPEmbeddings(model_name="ViT-L-14", checkpoint="laion2b_s32b_b82k")
```

---

## Quick Reference

| Method | Input | Output |
|--------|-------|--------|
| `embed_query(text)` | String | list[float] |
| `embed_documents(texts)` | list[str] | list[list[float]] |
| `embed_image(path)` | Path/URL | list[float] |
| `embed_image(paths)` | list[Path] | list[list[float]] |
