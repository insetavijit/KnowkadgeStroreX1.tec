| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.6.2.1 Storing Multi-Modal Vectors**     | Architecture                            | Storing text and image embeddings together                                    | Store mixed modality vectors in same collection.                |
| **LC11.6.2.2 Chroma for Multi-Modal**          | Implementation                          | Using Chroma with CLIP embeddings                                             | Chroma works seamlessly with CLIP.                              |
| **LC11.6.2.3 Metadata Strategies**             | Organization                            | Tagging documents by modality type                                            | Use metadata to distinguish text vs image docs.                 |
| **LC11.6.2.4 Separate vs Unified**             | Design                                  | One collection vs separate collections                                        | Unified enables cross-modal; separate is simpler.               |
| **LC11.6.2.5 Index Optimizations**             | Performance                             | Optimizing for multi-modal retrieval                                          | HNSW works well for CLIP vectors.                               |

# Multi-Modal Vector Stores: Unified Storage

Store text and images in the same searchable space.

---

## 1. Basic Multi-Modal Store

```python
from langchain_chroma import Chroma
from langchain_experimental.open_clip import OpenCLIPEmbeddings
from langchain_core.documents import Document

# Same embeddings for both modalities
clip = OpenCLIPEmbeddings()

# Create unified store
store = Chroma(
    collection_name="multimodal",
    embedding_function=clip,
    persist_directory="./mm_store"
)
```

---

## 2. Adding Documents

**Text Documents**:
```python
text_docs = [
    Document(
        page_content="A beautiful sunset over the ocean",
        metadata={"type": "text", "source": "descriptions.txt"}
    ),
    Document(
        page_content="Technical diagram of neural network",
        metadata={"type": "text", "source": "docs.md"}
    )
]

store.add_documents(text_docs)
```

**Image Documents**:
```python
# For images, page_content is empty (CLIP uses image directly)
image_docs = [
    Document(
        page_content="",  # Empty for images
        metadata={
            "type": "image",
            "path": "/images/sunset.jpg",
            "description": "Beach sunset photo"
        }
    )
]

# Add with custom IDs
store.add_documents(image_docs, ids=["img_001"])
```

---

## 3. Metadata Strategies

Track modality for filtering:

```python
# Add with modality metadata
def add_image(store, path: str, description: str = ""):
    doc = Document(
        page_content="",
        metadata={
            "modality": "image",
            "path": path,
            "description": description,
            "added_at": datetime.now().isoformat()
        }
    )
    store.add_documents([doc])

def add_text(store, text: str, source: str = ""):
    doc = Document(
        page_content=text,
        metadata={
            "modality": "text",
            "source": source,
            "added_at": datetime.now().isoformat()
        }
    )
    store.add_documents([doc])
```

**Filtered Search**:
```python
# Search only images
image_results = store.similarity_search(
    query="sunset",
    k=5,
    filter={"modality": "image"}
)

# Search only text
text_results = store.similarity_search(
    query="sunset",
    k=5,
    filter={"modality": "text"}
)
```

---

## 4. Unified vs Separate Collections

| Approach | Pros | Cons |
|----------|------|------|
| **Unified** | Cross-modal search works naturally | Mixed results may need sorting |
| **Separate** | Clear separation, simpler logic | Manual combination for cross-modal |

**Unified** (Recommended for cross-modal):
```python
store = Chroma(collection_name="unified", embedding_function=clip)
# Use metadata filters to separate when needed
```

**Separate**:
```python
text_store = Chroma(collection_name="texts", embedding_function=text_embeddings)
image_store = Chroma(collection_name="images", embedding_function=clip)
```

---

## 5. Index Configuration

For CLIP's high-dimensional vectors:

```python
# Chroma automatically uses HNSW
# For Pinecone, configure explicitly:
import pinecone

pinecone.create_index(
    name="multimodal",
    dimension=512,  # Match CLIP dimension
    metric="cosine",
    spec=pinecone.ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    )
)
```

---

## Quick Reference

| Operation | Method |
|-----------|--------|
| **Add Text** | `add_documents([Document(...)])` |
| **Add Image** | `add_documents([Document(page_content="", ...)])` |
| **Search All** | `similarity_search(query)` |
| **Filter** | `similarity_search(query, filter={...})` |
