| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.6.6.1 Multi-Modal RAG Architecture**    | Design                                  | RAG with multiple modality retrievers                                         | Retrieve text AND images for richer context.                    |
| **LC11.6.6.2 Parallel Retrieval**              | Strategy                                | Searching multiple stores simultaneously                                      | Search text and image stores in parallel.                       |
| **LC11.6.6.3 Result Fusion**                   | Combination                             | Combining results from different modalities                                   | Merge and rank cross-modal results.                             |
| **LC11.6.6.4 Context Assembly**                | Presentation                            | Building prompts with mixed modalities                                        | Assemble text + images into VLM prompt.                         |
| **LC11.6.6.5 Production Patterns**             | Real-world                              | Practical multi-modal RAG implementations                                     | Build production-ready multi-modal RAG.                         |

# Multi-Modal Retrieval: RAG with Images and Text

Retrieve both text documents and images for comprehensive context.

---

## 1. Multi-Modal RAG Architecture

```
Query → ┌─→ Text Retriever ──→ Text Docs ─┐
        │                                  ├─→ VLM ─→ Answer
        └─→ Image Retriever ──→ Images ───┘
```

Both modalities contribute to the context.

---

## 2. Parallel Retrieval

Search both stores simultaneously:

```python
import asyncio

async def parallel_retrieve(query: str, k: int = 5):
    """Retrieve from text and image stores in parallel."""
    
    text_task = asyncio.create_task(
        text_store.asimilarity_search(query, k=k)
    )
    image_task = asyncio.create_task(
        image_store.asimilarity_search(query, k=k)
    )
    
    text_results, image_results = await asyncio.gather(text_task, image_task)
    
    return {"text": text_results, "images": image_results}

# Usage
results = asyncio.run(parallel_retrieve("neural network architecture"))
```

---

## 3. Result Fusion

Combine and rank results:

```python
from dataclasses import dataclass

@dataclass
class RankedResult:
    content: any
    modality: str
    score: float
    metadata: dict

def reciprocal_rank_fusion(text_results: list, image_results: list, k: int = 60):
    """Combine results using Reciprocal Rank Fusion."""
    
    scores = {}
    
    for rank, doc in enumerate(text_results):
        doc_id = f"text:{doc.page_content[:50]}"
        scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)
    
    for rank, doc in enumerate(image_results):
        doc_id = f"image:{doc.metadata['path']}"
        scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)
    
    # Return sorted by combined score
    return sorted(scores.items(), key=lambda x: -x[1])
```

---

## 4. Context Assembly for VLM

Build multi-modal prompts:

```python
def build_multimodal_context(query: str, text_docs: list, images: list, max_images: int = 3):
    """Assemble context for VLM."""
    
    # Format text context
    text_context = "\n\n".join([doc.page_content for doc in text_docs])
    
    # Build message content
    content = [
        {"type": "text", "text": f"Context documents:\n{text_context}"},
    ]
    
    # Add images (limit to avoid token overflow)
    for img_doc in images[:max_images]:
        content.append({
            "type": "image_url",
            "image_url": {"url": encode_image(img_doc.metadata["path"])}
        })
    
    content.append({"type": "text", "text": f"\nQuestion: {query}"})
    
    return HumanMessage(content=content)
```

---

## 5. Complete Multi-Modal RAG Chain

```python
class MultiModalRAG:
    def __init__(self):
        self.text_embeddings = OpenAIEmbeddings()
        self.clip = OpenCLIPEmbeddings()
        
        self.text_store = Chroma(
            collection_name="texts",
            embedding_function=self.text_embeddings
        )
        self.image_store = Chroma(
            collection_name="images",
            embedding_function=self.clip
        )
        
        self.vlm = ChatOpenAI(model="gpt-4o")
    
    async def retrieve(self, query: str, k: int = 5):
        """Retrieve from both stores."""
        text_results = await self.text_store.asimilarity_search(query, k=k)
        image_results = await self.image_store.asimilarity_search(query, k=k)
        return text_results, image_results
    
    async def answer(self, query: str):
        """Full RAG pipeline."""
        
        # Parallel retrieval
        text_docs, image_docs = await self.retrieve(query)
        
        # Build context
        message = build_multimodal_context(query, text_docs, image_docs)
        
        # Generate answer
        response = await self.vlm.ainvoke([message])
        
        return {
            "answer": response.content,
            "text_sources": [d.metadata for d in text_docs],
            "image_sources": [d.metadata for d in image_docs]
        }

# Usage
rag = MultiModalRAG()
result = asyncio.run(rag.answer("Show me the network architecture"))
```

---

## Quick Reference

| Step | Action |
|------|--------|
| **Retrieve** | Search text and image stores |
| **Fuse** | Combine with RRF or scoring |
| **Assemble** | Build multi-modal message |
| **Generate** | Send to VLM for answer |
