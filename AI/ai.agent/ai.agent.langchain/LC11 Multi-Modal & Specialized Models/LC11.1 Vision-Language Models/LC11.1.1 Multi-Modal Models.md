| **Subtopic**                                   | **Focus & Purpose**                     | **Key Concepts / Details**                                                    | **One-Line Recall**                                             |
| ---------------------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **LC11.1.1.1 What is a VLM?**                  | Definition                              | Models that understand both text AND images simultaneously                    | VLMs see and read at the same time.                             |
| **LC11.1.1.2 Key Providers**                   | Ecosystem                               | GPT-4V (OpenAI), Claude 3 (Anthropic), Gemini (Google), LLaVA (Open Source)   | Major providers: GPT-4V, Claude 3, Gemini.                      |
| **LC11.1.1.3 Use Cases**                       | Applications                            | Document understanding, visual QA, image captioning, chart reading            | VLMs unlock document, chart, and image understanding.           |
| **LC11.1.1.4 Architecture Basics**             | How it works                            | Vision encoder + LLM decoder; image patches as tokens                         | Images become tokens via a vision encoder.                      |
| **LC11.1.1.5 LangChain Integration**           | Setup                                   | ChatOpenAI with gpt-4o; HumanMessage with image_url content                   | Use ChatOpenAI with image content blocks.                       |

# Multi-Modal Models: Seeing and Reading

Text-only LLMs are blind.
VLMs (Vision-Language Models) can SEE the image you provide.

---

## 1. What is a VLM?

A model that accepts **both** text and images as input.
It can answer questions about pictures, describe scenes, read text in photos (OCR), and analyze charts.

**Example**:
*   **Input**: An image of a pie chart + "What's the largest segment?"
*   **Output**: "Marketing, at 42%."

---

## 2. Key Providers

| Provider | Model | Notes |
|----------|-------|-------|
| **OpenAI** | `gpt-4o`, `gpt-4-vision-preview` | High accuracy, ~$5-10/1K images |
| **Anthropic** | `claude-3-opus`, `claude-3-sonnet`, `claude-3-haiku` | Strong reasoning |
| **Google** | `gemini-1.5-pro`, `gemini-1.5-flash` | Long context, video support |
| **Open Source** | LLaVA, Qwen-VL | Run locally, no API cost |

---

## 3. Architecture (Simplified)

```
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│   Image     │─────►│  Vision     │─────►│   LLM       │───► Text Output
│   (JPEG)    │      │  Encoder    │      │   Decoder   │
└─────────────┘      └─────────────┘      └─────────────┘
                     (Converts to          (Generates
                      Token-like            text response)
                      embeddings)
```

The Vision Encoder (like CLIP) converts images into a format the LLM can understand.

---

## 4. LangChain Integration

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI(model="gpt-4o")

message = HumanMessage(
    content=[
        {"type": "text", "text": "What is in this image?"},
        {"type": "image_url", "image_url": {"url": "https://example.com/cat.jpg"}}
    ]
)

response = llm.invoke([message])
print(response.content)
```

---

## Quick Reference

| Term | Meaning |
|------|---------|
| **VLM** | Vision Language Model |
| **Image Tokens** | How images are represented internally |
| **Content Block** | Multi-modal message format `[{type: "text"}, {type: "image_url"}]` |
